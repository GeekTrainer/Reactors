{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploration: Linear Regression and Classification\n",
        "\n",
        "A fundamental component of mastering data science concepts is applying and practicing them. This exploratory notebook is designed to provide you with a semi-directed space to do just that with the Python, linear regression, and ML-based classification skills that you either covered in an in-person workshop or through Microsoft Learn. The specific examples in this notebook apply NumPy and pandas concepts in a life-sciences context, but they are applicable across disciplines and industry verticals.\n",
        "\n",
        "This notebook is divided into different stages of exploration. Initial suggestions for exploration are more structured than later ones and can provide some additional concepts and skills for tackling data-science challenges with real-world data. However, this notebook is designed to provide you with a launchpad for your personal experimentation with data science, so feel free to add cells and running your own experiments beyond those suggested here. That is the power and the purpose of a platform like Jupyter Notebook!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Refresher on Notebooks\n",
        "\n",
        "Before we begin, you will need to important the principal libraries used to explore and manipulate data in Python: NumPy, pandas, and scikit-learn. The cell below also imports Matplotlib, the main visualization library in Python. For simplicity and consistency with prior instruction, industry-standard aliases are applied to these imported libraries. The cell below also runs `%matplotlib inline` magic command, which instructs Jupyter to display Matplotlib output directly in the notebook. This cell also imports many of the specific functions from scikit-learn that you will need, but feel free to import others as you see fit in the course of your exploration."
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "%matplotlib inline\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As it might have been a while since you last worked with Jupyter notebooks, here is a quick refresher on efficiently using them.\n",
        "\n",
        "### Notebook Cells\n",
        "\n",
        "Notebook cells are divided into Markdown text cells and interactive code cells. You can easily recognize code cells by the `[-]:` to the left of them.\n",
        "\n",
        "Code in a code cells has only been executed -- and is thus available for use in other code cells in the notebook -- if there is a number beside the code cell (for example, `[1]:`).\n",
        "\n",
        "To run the code in a cell, you can click the **Run** icon at the top left of a code cell or press **`Ctrl` + `Enter`**.\n",
        "\n",
        "### Documentation and Help\n",
        "\n",
        "Documentation for Python objects and functions is available directly in Jupyter notebooks. In order to access the documentation, simply put a question mark in front of the object or function in a code cell and execute the cell (for example, `?print`). A window containing the documentation will then open at the bottom of the notebook.\n",
        "\n",
        "On to exploration!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Guided Exploration\n",
        "\n",
        "For the first part of this workshop, you will step into the role of a data scientist examining some raw biological statistics. The dataset provided is in the `mammals.csv` file, which documents the body weight (in kilograms) and the brain weight (in grams) of 62 mammals.  (Source: Rogel-Salazar, Jesus (2015): Mammals Dataset. figshare. Dataset. https://doi.org/10.6084/m9.figshare.1565651.v1. Drawn from Allison, T. and Cicchetti, D. V. (1976). Sleep in mammals: ecological and constitutional correlates. *Science, 194*, 732â€“734.)\n",
        "\n",
        "Specifically, your task is to evaluate the relationship between mammalian body weight and brain weight. Even without domain expertise, it seems logical that some relationship should exist (afterall, it seems safe to assume that larger animals would have proporitionally larger brains). But what is the exact relationship? Any strong is that relationship. Determining those details is useful in fields such as evolutionary biology and doing so will be your job today.\n",
        "\n",
        "### Import and Investigate the Data\n",
        "\n",
        "Use `pd.read_csv()` to import `mammals.csv` and perform any other initial investigation you feel necessary in order to become familiar with the dataset. (For a refresher on importing data into pandas, see the Reactors modules on Manipulating and Cleaning Data or pandas or refer to the [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the data from mammals.csv into a DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot the Data\n",
        "\n",
        "Often the best way to get a sense of your data is to do so visually. Because you have to numerical features, a scatter plot would be most appropriate for this dataset. pandas DataFrames have [two](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html) [methods](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.scatter.html) that can be used to create scatter plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a scatterplot of mammalian body and brain masses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Don't worry: your scatter plot should be hard to read. Most of the values are clustered at the tiny end with two very large mammals in particular (the Asian and African elephants) skewing the scale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transform Your Data\n",
        "\n",
        "Because of the decidedly non-linear dispersion of mammalian size, you will need to transform you data in order to more clearly see the relationships in it.\n",
        "\n",
        "**Group or Partner Discussion**\n",
        "\n",
        " - Which feature in the dataset should you transform? Should you transform both? In either case, why? What do you predict you might see after the transformation?\n",
        " - What transformation do you think you should use? What key words do you see in the documentation for the pandas plot method that might give a hint?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a scatterplot of mammalian body and brain masses with base-10 logarithmic transformations applied to both axes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit and Plot a Linear Regression on Your Data\n",
        "\n",
        "Transformed, the data presents an elegant linear relationship; just looking at the scatterplot, the line practically draws itself. But how tight is that linear relationship? Put another way, how far away from the line of best fit are the points of your dataset on average from the line of best fit? Fit and plot a simple linear regression model for the data to find out. If you are unsure about how to do this, refer to the Reactors module on Machine Learning Models for a reminder. You will need to perform the following steps to do fit the model:\n",
        "\n",
        "1. Split your dataset into predictor variable (`X` is a common name of this variable) and the respons variable (`y` is a common variable name for this). (Remember to transform your data at this stage in the same way that you transformed it when you plotted it.)\n",
        "2. Further divide your into training and test subsets. (There is a scikit-learn function for this.)\n",
        "3. Create the linear regression model object.\n",
        "4. Fit the model to the training data.\n",
        "\n",
        "**Note:** You will get an error when you try to fit the data. Refer to https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.to_numpy.html and to https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.reshape.html for documenation on how to reshape the your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# You will need to reshape and transform your predictor (body) in order to fit your model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now plot your model with your data. Run [`matplotlib.pyplot.plot`](https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.plot.html) along with your transformed scatter plot of the data to do so. (Using a different color for the model will help you see it more clearly.)\n",
        "\n",
        "**Group or Partner Discussion**\n",
        "\n",
        " - Does your model plot as you expected it to? Does your linear model show up as a line? Remember that are modeling log-transformed data; what complementary transformations do you need to run on it to produce the linear plot you expect?\n",
        "\n",
        "**Note:** Python has functions that can perform the necessary transformations and the dataset is small enough that you will likely not notice a lag in performance by using the native Python functions. That said, it is a good habit to develop to use the NumPy ufuncs for when you deal with larger datasets. For a reminder on these, refer to the Reactors moduls on NumPy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the body and brain masses with base-10 logarithmic transformations applied to both axes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If your transformations were successful, you should see a gratifyingly tight line with your points of data closely clustered about it. But looks can be subjective. How good is relation in reality? Us the [$R^2$ score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) to find out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print out the R-squared value for you model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Intermediate Exploration\n",
        "\n",
        "As your investigation showed, there is a profound relationship between the size of mammals and the size of their brains: your $R^2$ score indicates that almost 88% of the proportional, average weight of a mammal's brain can be explained by its body weight alone. (The task of evolutionary biologists is to then examine what accounts for the other 12% of the variance.) A good model, to be sure, but in practice, how good is 'good'? Plot the linear model against the untransformed data to see this more clearly.\n",
        "\n",
        "To create this plot, you will again use the pandas `plot` (or `plot.scatter`) method in conjunction with `matplotlib.pyplot.plot`. However, you will need to create Numpy array to supply inputs along the x-axis for `matplotlib.pyplot.plot`. (See the Reactors Numpy module for a refresher on how to do that.) You will also need to transform that array when you input it to you model and further transform the output of your model to plot it accurately.\n",
        "\n",
        "**Group or Partner Discussion**\n",
        "\n",
        " - What errors are you getting? Do you need to reshape any of the data? Are zero values causing trouble for any of your transformations? How should you best deal with those?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the body and brain masses without the log transformations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Even with a good model, the noise inherent in real data can means that experimental values still diverge from your predicted ones by more than 20% in some cases. For this reason, it can be valuable to remember the maxim of statistician George Box that \"[all models are wrong, but some are useful](https://en.wikipedia.org/wiki/All_models_are_wrong).\"\n",
        "\n",
        "**Note:** Another way to approach this challenge is to deal with the model coefficient and intercept directly. Recall that linear models take the form of `Y`$ = $`intercept`$ + $`coefficient`$ * $`x`. Check the documentation for your model object to see how to access those values in your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Print out the coefficient and intercept for the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### When Linear Regression is Less Helpful\n",
        "\n",
        "As you develop your data-science skills, it is natural to want to apply the new tools that you learn to use on a variety of problems. Thus when learning about new algorithms, it can sometimes be as valuable to learn about when not to apply them as when to use them.\n",
        "\n",
        "For example, consider a case where linear regression might not provide the insight that you would like. To investigate this, import and plot the `lynx.csv` dataset, which contains annual numbers of lynx trappings for 1821â€“1934 in the Mackenzie River area of Canada. (Source: Campbell, M. J. and Walker, A. M. (1977). A Survey of statistical work on the Mackenzie River series of annual Canadian lynx trappings for the years 1821â€“1934 and a new analysis. *Journal of the Royal Statistical Society series A, 140*, 411â€“431. doi: [10.2307/2345277](http://doi.org/10.2307/2345277).)"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# Import the data from lynx.csv and plot it.\n",
        "# Note: The plot of this data should produce a cyclical pattern.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Your plot should show and interesting cyclical pattern to the data. A high peak in lynx numbers is followed by three smaller peaks every 9-10 years and then the pattern repeats itself. Ample food supply enables Mackenzie River lynx to reproduce to high numbers, after which the population plummets due to lack of food. The food supply gradually builds back up, enabling a repeat of the boom-and-bust population growth of the lynx.\n",
        "\n",
        "**Group or Partner Discussion**\n",
        "\n",
        " - Is a linear model appropriate for data like this? Why or why not? What do you suspect you might see if you attempt to fit a linear model to this data?\n",
        "\n",
        "Go ahead and fit and plot this data as you did for the mammals dataset above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the lynx data and the fitted linear model.\n",
        "# Remember to reshape your data in order to fit the regression model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now check the $R^2$ score for this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print out the R-squared score for this model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Group or Partner Discussion**\n",
        "\n",
        " - What does this $R^2$ score mean? What interpretation might it carry? (This [essay](https://web.maths.unsw.edu.au/~adelle/Garvan/Assays/GoodnessOfFit.html) from the University of New South Wales might help your discussion.)\n",
        " - Even if your $R^2$ score cannot explain the proportion of variance explained by your model, what information might it nonetheless provide?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classification\n",
        "\n",
        "Some practitioners say that 70% of problems in data science involve classificaiton. This figure is probably higher still in life sciences. In order to explore ML-based classification, let's return to a dataset you already encountered in the Reactors Manipulating and Cleaning Data module. Import the Python scikit-learn library and use an iconic dataset that every data scientist has seen hundreds of times: British biologist Ronald Fisher's *Iris* data set used in his 1936 paper \"The use of multiple measurements in taxonomic problems.\"\n",
        "\n",
        "You have already imported the scikit-learn library containing the `iris` dataset; you can access it using the `load_iris()` function. (Look at the `?load_iris` documentation for more information about this function; the data and target information is stored separately.) You might also find it helpful to create a DataFrame with the iris information in order to investigate it. (Check the Reactors pandas module for a refresher on how to load data into DataFrames.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Fisher iris data.\n",
        "\n",
        "# Now create a DataFrame of the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the Reactors Machine Learning module, you used the logistic regression and decision tree algorithms to classify observations into two categories, but there are many other kinds of classification algorithms that you will explore in this module.\n",
        "\n",
        "#### $K$-means Clustering\n",
        "\n",
        "$K$-means clustering is an example of unsupervised machine learning. Rather than having to train a model, the $k$-means algorithm examines all of the data to make a determination of which category to assign a particular observation. All that you have to do is supply the algorithm with the number of categories into which you want observations classified.\n",
        "\n",
        "Based on the number of species in the `iris` dataset, what is the most appropriate number of clusters to submit to the algorithm? Fit a $k$-means model for that number of clusters and measure its accuracy. (Consult the ?metrics.accuracy_score documentation for information on how to do this.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a k-means clustering object with 3 clusters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Group or Partner Discussion**\n",
        "\n",
        " - Is this accuracy surprising? Try different values for the `random_state` parameter in the `KMeans` function (such as 0, 1, 2). Why the large disparities in accuracy based on the random state of the algorithm? (In the Individual Exploration section below, you can explore some of the structure of the `iris` dataset that can help generate these disparities.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### $K$-nearest Neighbors\n",
        "\n",
        "A classification algorithm that might work better on the `iris` dataset is the $k$-nearest neighbors algorithm (abbreviated $k$-NN). It works by comparing an observation to its $k$ nearest training observations in feature space (where $k$ is a parameter supplied by the user). $k$-NN is a supervised algorithm, so it does need to be supplied with a *response*--the correct classifications that you are looking for (often referred to as $y$)--in order to classify new observations. Fit a $k$-NN model and use `metrics.accuracy_score` to examine its accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into training and test subsets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Group or Partner Discussion**\n",
        "\n",
        " - Try supplying different `random_state` parameters to the `train_test_split` function (such as `random_state=0` and `random_state=2`). What causes the differences in accuracy?\n",
        "\n",
        "One way to mitigate the luck of the draw inherent in training/test splitting is to do so repeatedly. Testing your models in this way is called cross validation (or $k$-fold cross validation after the number of times you resplit the data, the folds). Scikit-learn has a [good page](https://scikit-learn.org/stable/modules/cross_validation.html) in its documentation on the concept.\n",
        "\n",
        "Use the `cross_val_score` model to perform a 10-fold cross validation on your $k$-NN model and take the mean of the accuracy scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run 10-fold cross-validation on the 5-nearest neighbor model you have already fitted.\n",
        "\n",
        "# Print out the mean score for the cross-validation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Individual Exploration\n",
        "\n",
        "Here are some ideas to continue your exploration of classification and predictive ML algorithms:\n",
        "\n",
        "- What number of nearest neighbors provides the highest average accuracy for your $k$-NN model? While it's true that with the $k$-NN algorithm you don't have to worry about feature engineering or selection, the selection of $k$ can play a big role in the algorithm's accuracy for you dataset and must be tuned. The better that you can tune your algorithm for accuracy, the better you can help domain experts solve problems, be they correctly identifying irises (in this example) or sequencing genes in bioinformatics or correctly identifying diseased cells in a medical application.\n",
        "\n",
        "(Hint: Try referring back to the Reactors Python module for a refresher on loops and data structures to see how you could automate this comparison over many several different values of $k$.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Create a list of values of k from 1 to 25.\n",
        "\n",
        "# Create a dictionary of k values and associated mean cross-validation accuracy for each value of k.\n",
        "\n",
        "# Find and print out the value of k that produced the highest accuracy and what that score is.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " - Visualize the `iris` dataset to see why the $k$-means algorithm produced the accuracy that it did.\n",
        " \n",
        " The `iris` dataset has four features, which means that a true scatter plot of all of the observations in it would require four dimensions, which is impossible to visualize directly. However, you can use a technique called principle component analysis (PCA) to reduce this to three dimensions with minimal loss of information. (Don't worry about the details of how PCA works; you will cover it in another session in the Reactors PCA module.)\n",
        " \n",
        " To visualized a \"flattened\" `iris` dataset, you will need to fit a PCA transformation with the data from the dataset. You can get some ideas about what code to use from this [page](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) from the scikit-learn documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform the PCA dimension-reduction on the iris data.\n",
        "\n",
        "# Create the 3D plot of the \"flattened\" iris data set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With this visualization in hand, why might $k$-means clustering not be a good classification algorithm to use with the `iris` dataset? (This [Wikipedia article](https://en.wikipedia.org/wiki/K-means_clustering) on $k$-means clustering might help your discussion.)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3-final",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}