{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project\n",
    "\n",
    "In this Capstone Project we will be engaging with the stock price data for several hundred stocks over 5 years, which can be found at:\n",
    "(./Data/prices-split-adjusted.csv.zip).  The data is from [Kaggle](https://www.kaggle.com/dgawlik/nyse#prices-split-adjusted.csv) and is available in its original form there under a CC0 license. We will be using a slightly preprocessed version in the repo.  \n",
    "\n",
    "In this notebook we will be looking at financial forecasting with machine learning.  This is inherently one of the hardest problems in machine learning, because some of the most advanced and well-funded technical teams in the world are trying to use machine learning and other techniques to find patterns in the financial data.  When they find patterns and if they trade on those findings, prices will move in a way that makes those patterns less pronounced over time. This is not to say that this is not a fun and rewarding area.  Just do not get discouraged if you don't find an instant money machine.  \n",
    "\n",
    "### Outline:\n",
    "0. Background\n",
    "\n",
    "1. Preparing our tools\n",
    "\n",
    "2. Importing and describing the data.\n",
    "\n",
    "3. Exploring, cleaning and visualizing the data\n",
    "\n",
    "3. Developing analytics\n",
    "\n",
    "4. Preparing and splitting our data\n",
    "\n",
    "5. Building our first model\n",
    "\n",
    "6. Extending to other ML models\n",
    "\n",
    "7. Ideas for further strategies\n",
    "\n",
    "8. Wrapping up\n",
    "\n",
    "### Options\n",
    "As we progress you are encouraged to take this dataset further. You are also encouraged to explore any aspects of the data. Develop your own algorithms. Be explicit about your inquiry and success in predicting effects on our world.\n",
    "\n",
    "### Warning: Not financial advice\n",
    "This exercise is meant purely for educational purposes, uses many simplifications and is not intended, nor should be considered as financial advice. There are many risks involved in implementation of financial trading strategies that are not considered nor described here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up\n",
    "If you have not yet set up your environment, you can easily do so with VS Code, and the python extension and Anaconda.  \n",
    "\n",
    "For VSCode go here: [https://code.visualstudio.com/]\n",
    "\n",
    "and then you can follow these instructions:\n",
    "[https://code.visualstudio.com/docs/python/data-science-tutorial]\n",
    "\n",
    "0. Background\n",
    "\n",
    "Machine learning is of increasing importance in finance. As volumes of data grow ever faster, the need for machine driven models to find patterns in that data becomes ever more important.  In the ever-accelerating race to better process data into predictions about securities prices, machine learning has become an important tool, because it is good at finding patterns in large amounts of data.  Today we will be examining patterns in stock prices themselves to practice developing models to predict future set prices. If there are systematic trends, patterns or reversals, then we may detect them. \n",
    "\n",
    "While the chances that we discover totally new and unexploited price patterns today is low, we will practice organizing our data, creating and analyzing machine learning models that will give us the tools to develop state of the art signals of value.\n",
    "\n",
    "Goals: \n",
    "\n",
    "1. Become familiar and practice the process of building machine learning models as they relate to financial data.  \n",
    "\n",
    "2. Understand the special processing that is required when working with time series data such those found in finance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparing our tools.\n",
    "\n",
    "Let's review our standard imports:\n",
    "- numpy for rapid numerical calculations with fast vectorized C implementations\n",
    "- pandas for processing data\n",
    "- matplotlib and Seaborn for visualizing charts\n",
    "- scikit-learn (imported as sklearn) is the de facto standard machine learning library in the pydata ecosystem.  \n",
    " \n",
    "Additionally, we will be using [pandas_profiling](https://github.com/pandas-profiling/pandas-profiling) which is a newer convenience package that helps by putting together much of our initial *boilerplate* exploratory data analysis code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring our tools in:\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and describing the data\n",
    "\n",
    "Now we are ready to import our data.  The value rows can be set if you only want to import a small subset due to computer memory or speed constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using information from the data source description to know that date is a column containing just what is says\n",
    "rows = None\n",
    "stocks = pd.read_csv('./Data/prices-split-adjusted.csv.zip', nrows=rows, parse_dates=['date'], index_col='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data successfully loaded, let's explore what we have. First, summarize the dataframe via the info method to validate the data reading and parsing.  When looking at the info report, it is best practice to note that each column is the expected type, noting that strings are reported as object.  Also note if there are null values, how many values there are and what the columns are.  \n",
    "\n",
    "We do not have a data dictionary in this case.  But if you were lucky enough to have access to a data dictionary, this is a good time to check that the dictionary matches what you actually have.  Discrepancies could be the result of mis-parsing, undocumented schema changes, documentation that is not up to date, or a number of other reasons.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 851264 entries, 2016-01-05 to 2016-12-30\nData columns (total 6 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   symbol  851264 non-null  object \n 1   open    851264 non-null  float64\n 2   close   851264 non-null  float64\n 3   low     851264 non-null  float64\n 4   high    851264 non-null  float64\n 5   volume  851264 non-null  float64\ndtypes: float64(5), object(1)\nmemory usage: 45.5+ MB\n"
    }
   ],
   "source": [
    "stocks.info() # Look at the descriptions of the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks as expected in info.  The string column symbol is the trading symbol, also known in finance as the ticker.  The dates were parsed as expected and all the other columns are numeric.  Now we can look at the first few rows of data to get a sample view.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "           symbol        open       close         low        high     volume\ndate                                                                        \n2016-01-05   WLTW  123.430000  125.839996  122.309998  126.250000  2163600.0\n2016-01-06   WLTW  125.239998  119.980003  119.940002  125.540001  2386400.0\n2016-01-07   WLTW  116.379997  114.949997  114.930000  119.739998  2489500.0\n2016-01-08   WLTW  115.480003  116.620003  113.500000  117.440002  2006300.0\n2016-01-11   WLTW  117.010002  114.970001  114.089996  117.330002  1408600.0\n2016-01-12   WLTW  115.510002  115.550003  114.500000  116.059998  1098000.0\n2016-01-13   WLTW  116.459999  112.849998  112.589996  117.070000   949600.0\n2016-01-14   WLTW  113.510002  114.379997  110.050003  115.029999   785300.0\n2016-01-15   WLTW  113.330002  112.529999  111.919998  114.879997  1093700.0\n2016-01-19   WLTW  113.660004  110.379997  109.870003  115.870003  1523500.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>symbol</th>\n      <th>open</th>\n      <th>close</th>\n      <th>low</th>\n      <th>high</th>\n      <th>volume</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2016-01-05</th>\n      <td>WLTW</td>\n      <td>123.430000</td>\n      <td>125.839996</td>\n      <td>122.309998</td>\n      <td>126.250000</td>\n      <td>2163600.0</td>\n    </tr>\n    <tr>\n      <th>2016-01-06</th>\n      <td>WLTW</td>\n      <td>125.239998</td>\n      <td>119.980003</td>\n      <td>119.940002</td>\n      <td>125.540001</td>\n      <td>2386400.0</td>\n    </tr>\n    <tr>\n      <th>2016-01-07</th>\n      <td>WLTW</td>\n      <td>116.379997</td>\n      <td>114.949997</td>\n      <td>114.930000</td>\n      <td>119.739998</td>\n      <td>2489500.0</td>\n    </tr>\n    <tr>\n      <th>2016-01-08</th>\n      <td>WLTW</td>\n      <td>115.480003</td>\n      <td>116.620003</td>\n      <td>113.500000</td>\n      <td>117.440002</td>\n      <td>2006300.0</td>\n    </tr>\n    <tr>\n      <th>2016-01-11</th>\n      <td>WLTW</td>\n      <td>117.010002</td>\n      <td>114.970001</td>\n      <td>114.089996</td>\n      <td>117.330002</td>\n      <td>1408600.0</td>\n    </tr>\n    <tr>\n      <th>2016-01-12</th>\n      <td>WLTW</td>\n      <td>115.510002</td>\n      <td>115.550003</td>\n      <td>114.500000</td>\n      <td>116.059998</td>\n      <td>1098000.0</td>\n    </tr>\n    <tr>\n      <th>2016-01-13</th>\n      <td>WLTW</td>\n      <td>116.459999</td>\n      <td>112.849998</td>\n      <td>112.589996</td>\n      <td>117.070000</td>\n      <td>949600.0</td>\n    </tr>\n    <tr>\n      <th>2016-01-14</th>\n      <td>WLTW</td>\n      <td>113.510002</td>\n      <td>114.379997</td>\n      <td>110.050003</td>\n      <td>115.029999</td>\n      <td>785300.0</td>\n    </tr>\n    <tr>\n      <th>2016-01-15</th>\n      <td>WLTW</td>\n      <td>113.330002</td>\n      <td>112.529999</td>\n      <td>111.919998</td>\n      <td>114.879997</td>\n      <td>1093700.0</td>\n    </tr>\n    <tr>\n      <th>2016-01-19</th>\n      <td>WLTW</td>\n      <td>113.660004</td>\n      <td>110.379997</td>\n      <td>109.870003</td>\n      <td>115.870003</td>\n      <td>1523500.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "stocks.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the head gives a good preview of a piece of the data, it may not be a great overall view of the entire dataset, especially for larger data sets or ones that may have been sorted at some point.  However, we are more comfortable that the dates were parsed correctly.  We can investigate numerical columns with describe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                open          close            low           high  \\\ncount  851264.000000  851264.000000  851264.000000  851264.000000   \nmean       64.993618      65.011913      64.336541      65.639748   \nstd        75.203893      75.201216      74.459518      75.906861   \nmin         1.660000       1.590000       1.500000       1.810000   \n25%        31.270000      31.292776      30.940001      31.620001   \n50%        48.459999      48.480000      47.970001      48.959999   \n75%        75.120003      75.139999      74.400002      75.849998   \nmax      1584.439941    1578.130005    1549.939941    1600.930054   \n\n             volume  \ncount  8.512640e+05  \nmean   5.415113e+06  \nstd    1.249468e+07  \nmin    0.000000e+00  \n25%    1.221500e+06  \n50%    2.476250e+06  \n75%    5.222500e+06  \nmax    8.596434e+08  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>close</th>\n      <th>low</th>\n      <th>high</th>\n      <th>volume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>851264.000000</td>\n      <td>851264.000000</td>\n      <td>851264.000000</td>\n      <td>851264.000000</td>\n      <td>8.512640e+05</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>64.993618</td>\n      <td>65.011913</td>\n      <td>64.336541</td>\n      <td>65.639748</td>\n      <td>5.415113e+06</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>75.203893</td>\n      <td>75.201216</td>\n      <td>74.459518</td>\n      <td>75.906861</td>\n      <td>1.249468e+07</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.660000</td>\n      <td>1.590000</td>\n      <td>1.500000</td>\n      <td>1.810000</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>31.270000</td>\n      <td>31.292776</td>\n      <td>30.940001</td>\n      <td>31.620001</td>\n      <td>1.221500e+06</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>48.459999</td>\n      <td>48.480000</td>\n      <td>47.970001</td>\n      <td>48.959999</td>\n      <td>2.476250e+06</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>75.120003</td>\n      <td>75.139999</td>\n      <td>74.400002</td>\n      <td>75.849998</td>\n      <td>5.222500e+06</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1584.439941</td>\n      <td>1578.130005</td>\n      <td>1549.939941</td>\n      <td>1600.930054</td>\n      <td>8.596434e+08</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "stocks.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Pandas Profiling\n",
    "Data exploration can start with a turnkey tool like pandas-profiling.  The important this with this is to make sure you actually look at the report and digest the output.  Make it the start of your investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Tab(children=(HTML(value='<div id=\"overview-content\" class=\"row variable spacing\">\\n    <div class=\"row\">\\n   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f1e9a3f6d3841b3a731d32e0521df8f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Report generated with <a href=\"https://github.com/pandas-profiling/pandas-profiling\">pandas-profiling</a>."
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Minimal avoids expensive calculations that won't have much insight for us and are slow.\n",
    "profile = ProfileReport(stocks, minimal=True)\n",
    "profile.to_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: \n",
    "\n",
    "Examine this report and see what insights you notice.  What do you notice about the data?  Are there ways to slice the data that would give more information?    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blank cell left for student exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at an individual stock gives a much clearer impression of the distributions of each column.  Even if you can not do this for every stock, taking a sample can be very helpful.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploring, cleaning and visualizing the data\n",
    "\n",
    "#### Modeling our data\n",
    "One of the most important steps in preparing your data is to think of how we want to think about it.  For those who have worked with SQL, this is like identifying what the key to your table is. For this data exploration, we can think of the index into the data as being a a compound key of both the symbol (the ticker) and the date.  We will process the data in a way that will make a time series model for each stock.  Predicting the future based on what has happened in the past for that individual stock.  \n",
    "\n",
    "Thought Question: Can you think of other ways that you might want to consider this data?  What questions might prompt you to think of this data having a different data model?  The data model is not fixed but is a lens that lets us look at our data.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing our data\n",
    "It is often helpful to look at our data visually to see if there are any issues that \"look funny.\"  With experience, just looking at the data can help us understand it in a short amount of time.  This is often the step where domain expertise (in this case the financial markets) is especially useful.  Be sure to look at the histograms of the report above to see if they make sense to you.  \n",
    "\n",
    "Exercise: \n",
    "Are there other visualizations that would give you greater understanding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left blank for student answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature engineering\n",
    "For modelling data with machine learning, it is helpful to transform the data into a form that is closer to the theoretical expectations where the ML models should perform well. Let's transform the data into returns and generate other features.  We will transform returns with logarithms based on financial research that log returns are closer to normally distributed and (statistically) stable. \n",
    "\n",
    "The function below is just a sample of feature transformations.  Taking the logarithms can help deal with skewed data as we saw we have in the pandas-profile report.\n",
    "\n",
    "To be honest, which variables you use and how you transform them is largely dependent on domain expertise and traditions of the field. It can also be a matter of trial and error, although that can lead to overfitting.  We will discuss overfitting a little bit later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_target_generation(df):\n",
    "    \"\"\"\n",
    "    df: a pandas dataframe containing numerical columns\n",
    "    num_days_ahead: an integer that can be used to shift the prediction value from the future into a prior row.\n",
    "    \"\"\"\n",
    "\n",
    "    # The following line ensures the data is in date order    \n",
    "    features = pd.DataFrame(index=df.index).sort_index() \n",
    "    features['f01'] = np.log(df.close / df.open) # intra-day log return\n",
    "    features['f02'] = np.log(df.open / df.close.shift(1)) # overnight log return\n",
    "\n",
    "    features['f03'] = df.volume # try both regular and log volume\n",
    "    features['f04'] = np.log(df.volume) \n",
    "    features['f05'] = df.volume.diff() # 1-day absolute change in volume\n",
    "    features['f06'] = df.volume.pct_change() # 1-day relative change in volume\n",
    "    # The following are rolling averages of different periods\n",
    "    features['f07'] = df.volume.rolling(5, min_periods=1).mean().apply(np.log)\n",
    "    features['f08'] = df.volume.rolling(10, min_periods=1).mean().apply(np.log)\n",
    "    features['f09'] = df.volume.rolling(30, min_periods=1).mean().apply(np.log)\n",
    "\n",
    "    # More of our original data: low, high and close\n",
    "    features['f10'] = df.low \n",
    "    features['f11'] = df.high\n",
    "    features['f12'] = df.close\n",
    "    # The Intraday trading spread measures how far apart the high and low are\n",
    "    features['f13'] = df.high - df.low \n",
    "    # These are log returns over different time periods \n",
    "    features['f14'] = np.log(df.close / df.close.shift(1)) # 1 day log return\n",
    "    features['f15'] = np.log(df.close / df.close.shift(5)) # 5 day log return\n",
    "    features['f16'] = np.log(df.close / df.close.shift(10)) # 10 day log return\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Machine Learning, we need to predict something, typically called our target or predictor. \n",
    "\n",
    "Let's predict the value of the stock price 10 days into the future, using \"prediction_horizon\".  Here 10 is a hyperparameter that is somewhat arbitrary.  We may want to try different horizons to see if we are better at predicting the near future or the long term.  \n",
    "\n",
    "The ticker lets us start by testing on a single stock for speed in training.  \n",
    "\n",
    "We want to look at different periods of history to see how we do.  We will use overlapping sets with n_splits.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a list of tickers so we can easily select them\n",
    "ticker_list = stocks.symbol.unique()\n",
    "\n",
    "# these are hyperparameters you can play with or tune\n",
    "prediction_horizon = -5 # this is a negative number by convention\n",
    "ticker = 'MSFT' # choose any ticker\n",
    "n_splits = 5 \n",
    "\n",
    "# Make an individual model for each ticker/symbol\n",
    "features = feature_target_generation(stocks[stocks.symbol==ticker])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Preparing and splitting our data\n",
    "It is important that we separate our training and test data.  Since our rows are already time ordered, we can easily do splits.  This is one of the areas where times series data is different from other machine learning problems.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "TimeSeriesSplit(max_train_size=None, n_splits=5)\n"
    }
   ],
   "source": [
    "# We are trying to predict the price prediction_horizon days in the future.  So we take the future value and move it prediction_horizon into the past to line up our data in the Scikit-learn format.  \n",
    "y = features.f12.shift(prediction_horizon)\n",
    "# The latest (prediction_horizon) rows will have nans because we have no future data, so let's drop them.\n",
    "shifted = ~np.isnan(y)\n",
    "X = features[y.notna()] # Remove the rows that do not have valid target values\n",
    "y = y[shifted] # Remove the rows that do not have valid target values\n",
    "\n",
    "# Split the history into different backtesting regimes\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "print(tscv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 1762 entries, 2010-01-04 to 2016-12-30\nData columns (total 16 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   f01     1762 non-null   float64\n 1   f02     1761 non-null   float64\n 2   f03     1762 non-null   float64\n 3   f04     1762 non-null   float64\n 4   f05     1761 non-null   float64\n 5   f06     1761 non-null   float64\n 6   f07     1762 non-null   float64\n 7   f08     1762 non-null   float64\n 8   f09     1762 non-null   float64\n 9   f10     1762 non-null   float64\n 10  f11     1762 non-null   float64\n 11  f12     1762 non-null   float64\n 12  f13     1762 non-null   float64\n 13  f14     1761 non-null   float64\n 14  f15     1757 non-null   float64\n 15  f16     1752 non-null   float64\ndtypes: float64(16)\nmemory usage: 234.0 KB\n"
    }
   ],
   "source": [
    " # Review the features\n",
    " features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Building our first model\n",
    "This is a regression problem.  Why is that so? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression\n",
    "In our ML framework we can use linear regression, just as in standard statistics or econometrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_ts_report(model, tscv, X, y, impute=False):\n",
    "    \"\"\"\n",
    "    Fit the model and then run time series backtests.\n",
    "    \"\"\"\n",
    "    # Loop through the backtests\n",
    "    for train_ind, test_ind in tscv.split(X): \n",
    "        # Report on the time periods\n",
    "        print(f'Train is from {X.iloc[train_ind].index.min()} to {X.iloc[train_ind].index.max()}. ')\n",
    "        print(f'Test is from {X.iloc[test_ind].index.min()} to {X.iloc[test_ind].index.max()}. ')\n",
    "        # Generate training and testing features and target for each fold.\n",
    "        X_train, X_test = X.iloc[train_ind], X.iloc[test_ind]\n",
    "        y_train, y_test = y.iloc[train_ind], y.iloc[test_ind]\n",
    "\n",
    "        if impute==True:\n",
    "            # Since linear regression cannot deal with NaN, we need to impute.  There may be the better choices.\n",
    "            X_train.fillna(0, inplace=True)\n",
    "            X_test.fillna(0, inplace=True)\n",
    "        \n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict and measure on the training data\n",
    "        y_pred_train = model.predict(X_train) \n",
    "        print(\"Training results:\")\n",
    "        print(\"RMSE:\", mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "\n",
    "        # Predict and measure on the testing data\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        print(\"Test results:\")\n",
    "        print(\"RMSE:\", mean_squared_error(y_test, y_pred_test, squared=False))\n",
    "        print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train is from 2010-01-04 00:00:00 to 2011-03-08 00:00:00. \nTest is from 2011-03-09 00:00:00 to 2012-05-03 00:00:00. \nTraining results:\nRMSE: 0.7646691835724355\nTest results:\nRMSE: 0.8845465388255741\n\nTrain is from 2010-01-04 00:00:00 to 2012-05-03 00:00:00. \nTest is from 2012-05-04 00:00:00 to 2013-07-03 00:00:00. \nTraining results:\nRMSE: 0.8046921042806601\nTest results:\nRMSE: 0.9584478611049053\n\nTrain is from 2010-01-04 00:00:00 to 2013-07-03 00:00:00. \nTest is from 2013-07-05 00:00:00 to 2014-08-29 00:00:00. \nTraining results:\nRMSE: 0.8388520878621472\nTest results:\nRMSE: 1.219972645569116\n\nTrain is from 2010-01-04 00:00:00 to 2014-08-29 00:00:00. \nTest is from 2014-09-02 00:00:00 to 2015-10-27 00:00:00. \nTraining results:\nRMSE: 0.9135655974444156\nTest results:\nRMSE: 1.857836380010567\n\nTrain is from 2010-01-04 00:00:00 to 2015-10-27 00:00:00. \nTest is from 2015-10-28 00:00:00 to 2016-12-22 00:00:00. \nTraining results:\nRMSE: 1.1573573014112757\nTest results:\nRMSE: 1.5623450539790187\n\n"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# Fit and report on a linear model\n",
    "lm = LinearRegression()\n",
    "model_ts_report(lm, tscv, X, y, impute=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we look at our results, we can see that in each period we do better in training than in testing.  That is typical of finance and machine learning more generally. \n",
    "Interestingly, we are able to explain 90% of the variance with our first linear model.  \n",
    "\n",
    "If you have questions about the root-mean-squared-error (RMSE), please see the Microsoft learn module on machine learning.\n",
    "\n",
    "#### Ensemble Model\n",
    "Let's try a Random Forest, which is a commonly used model that blends a group of decision trees, each of which have access to a sub-sample of features.  It is commonly used because it it tends to work well with relatively little tuning of hyperparameters and is somewhat less likely to overfit.  This is NOT a classic model commonly used in econometrics, mainly because it does not have a linear structure that corresponds with econometric theory.  However, it is often used in predictions for items other than finance.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train is from 2010-01-04 00:00:00 to 2011-03-08 00:00:00. \nTest is from 2011-03-09 00:00:00 to 2012-05-03 00:00:00. \nTraining results:\nRMSE: 0.23049077731828058\nTest results:\nRMSE: 1.1669160674539722\n\nTrain is from 2010-01-04 00:00:00 to 2012-05-03 00:00:00. \nTest is from 2012-05-04 00:00:00 to 2013-07-03 00:00:00. \nTraining results:\nRMSE: 0.24843440252946497\nTest results:\nRMSE: 1.3551951722514546\n\nTrain is from 2010-01-04 00:00:00 to 2013-07-03 00:00:00. \nTest is from 2013-07-05 00:00:00 to 2014-08-29 00:00:00. \nTraining results:\nRMSE: 0.25784945977455104\nTest results:\nRMSE: 5.295733162857246\n\nTrain is from 2010-01-04 00:00:00 to 2014-08-29 00:00:00. \nTest is from 2014-09-02 00:00:00 to 2015-10-27 00:00:00. \nTraining results:\nRMSE: 0.27349224952765466\nTest results:\nRMSE: 2.944342872231963\n\nTrain is from 2010-01-04 00:00:00 to 2015-10-27 00:00:00. \nTest is from 2015-10-28 00:00:00 to 2016-12-22 00:00:00. \nTraining results:\nRMSE: 0.38488931689432365\nTest results:\nRMSE: 5.006480972041069\n\n"
    }
   ],
   "source": [
    "# Initiate a Random Forest\n",
    "rf = RandomForestRegressor()\n",
    "model_ts_report(rf, tscv, X, y, impute=True) # Report on the random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using a Random Forest, we are able to bring down our training data error metrics by a lot (over 50% decrease on RMSE)!  However, our test results are not as good for most of the time slices.  This is an indication that we are *overfitting* our model to the training data.  This model would likely not do as well in production as it would in our backtests.  Why?  Because our models have not done well on any of the new data, outside the time period during which they were trained. \n",
    "\n",
    "### Open Ended Exercise\n",
    "\n",
    "Now is your turn to go ahead and improve these models.  Some areas that might help could be to: \n",
    "- Tune the existing models (Random forest has a number of parameters that may help) \n",
    "- Clean the existing data (Fill missing values better) \n",
    "- Try other models such as Support Vector Regressor, Extra Trees Regressor or ElasticNet \n",
    "- Try this for more stocks (Just becasue it did not work for one stock, it may still be useful for most stocks) \n",
    "- Get more features, through transformations or outside data  \n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('reactor01': conda)",
   "language": "python",
   "name": "python38164bitreactor01conda043b4c7103914cefbaffebddbd23de07"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}