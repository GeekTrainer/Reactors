{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration: Multiple Regression and Principal Component Regression (PCR) \n",
    "\n",
    "A fundamental component of mastering data science concepts is applying and practicing them. This exploratory notebook is designed to provide you with a semi-directed space to do just that with the Python, PCA, and ML-based classification, and visualization skills that you either covered in an in-person workshop or through Microsoft Learn. The specific examples in this notebook apply Python and Pandas concepts in a life-sciences context, but they are applicable across disciplines and industry verticals.\n",
    "\n",
    "This notebook is divided into different stages of exploration. Initial suggestions for exploration are more structured than later ones and can provide some additional concepts and skills for tackling data-science challenges with real-world data. However, this notebook is designed to provide you with a launchpad for your personal experimentation with data science, so feel free to add cells and running your own experiments beyond those suggested here. That is the power and the purpose of a platform like Jupyter Notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Refresher on Notebooks\n",
    "\n",
    "Before we begin, you will need to import the principal libraries used to explore and manipulate data in Python: NumPy, pandas, and scikit-learn. The cell below also imports Matplotlib, the main visualization library in Python. For simplicity and consistency with prior instruction, industry-standard aliases are applied to these imported libraries. The cell below also runs `%matplotlib inline` magic command, which instructs Jupyter to display Matplotlib output directly in the notebook. This cell also imports many of the specific functions from scikit-learn that you will need, but feel free to import others as you see fit in the course of your exploration.\n",
    "\n",
    "If you get any errors such as:\n",
    "```Python\n",
    "ModuleNotFoundError: No module named 'statsmodels'\n",
    "```\n",
    "\n",
    "You might need to install the particular libraries into your Python environment. In Visual Studio Code you can open the terminal and if you have pip installed you can run:\n",
    "```Python\n",
    "pip install statsmodels\n",
    "```\n",
    "\n",
    "And then re-run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it might have been a while since you last worked with Jupyter notebooks, here is a quick refresher on efficiently using them.\n",
    "\n",
    "### Notebook Cells\n",
    "\n",
    "Notebook cells are divided into Markdown text cells and interactive code cells. You can easily recognize code cells by the `[-]:` to the left of them.\n",
    "\n",
    "Code in a code cells has only been executed -- and is thus available for use in other code cells in the notebook -- if there is a number beside the code cell (for example, `[1]:`).\n",
    "\n",
    "To run the code in a cell, you can click the **Run** icon at the top left of a code cell or press **`Ctrl` + `Enter`**.\n",
    "\n",
    "### Documentation and Help\n",
    "\n",
    "Documentation for Python objects and functions is available directly in Jupyter notebooks. In order to access the documentation, simply put a question mark in front of the object or function in a code cell and execute the cell (for example, `?print`). A window containing the documentation will then open at the bottom of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "In this workshop, you will step into the role of a clinical medical researcher. Specifically, you are tasked with building both exploratory and predictive models based off of anonymized patient medical data using multiple regression. However, your dataset might have too many features to include all of them in your model, so you will have to determine which features provide the most predictive power.\n",
    "\n",
    "##  Dataset\n",
    "\n",
    "The dataset that you have been asked to use comes from a study that examined the correlation between the level of prostate-specific antigen and a number of clinical measures in men who were about to receive a radical prostatectomy. The dataset has the follwing components:\n",
    "\n",
    " - **lcavol**: Logarithm of the cancer volume\n",
    " - **lweight**: Logarithm of the prostate weight\n",
    " - **age**: Patient age\n",
    " - **lbph**: Logarithm of the benign prostatic hyperplasia amount\n",
    " - **svi**: Seminal vesicle invasion (the presence of prostate cancer in the muscular tissue around the seminal vesicles and outside the prostate)\n",
    " - **lcp**: Logarithm of the capsular penetration (cancer that has reached the outer wall of the prostate)\n",
    " - **gleason**: The Gleason Score, which ranges from 1-5 and describes how much the cancer from a biopsy looks like healthy tissue (lower score) or abnormal tissue (higher score)\n",
    " - **pgg45**: Percentage Gleason scores 4 or 5\n",
    " - **lpsa**: Logarithm of the amount of prostate-specific antigen\n",
    "\n",
    "### Reference\n",
    "Stamey, T.A., Kabalin, J.N., McNeal, J.E., Johnstone, I.M., Freiha, F., Redwine, E.A. and Yang, N. (1989). Prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the prostate: II. radical prostatectomy treated patients. *Journal of Urology* **141**(5), 1076â€“1083. The dataset is available for download [here](https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data) (in addition to other places).\n",
    "\n",
    "## Section 1: Guided Exploration\n",
    "\n",
    "The first task in any data role is easy: import the data into a DataFrame and quickly look over it to see what you are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from prostate.csv.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has nine features, which is on the boundary for the maximum number of predictors we can use before overfitting becomes and issue. We worry about overfitting models because such models can behave very well on training data but then work very poorly with new data. Not great for prediciting prostate-specific antigen levels in patients! This means that we need to figure out which of these features will produce the most predictive power in a model.\n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "You could approach this problem by manually building models and then comparing them, but this would be ruinously inefficient. Instead, you will create and automated means of testing hundreds of models to find a best model.\n",
    "\n",
    "Before building out the automation, separate out your data into your predictors and response. Remember that you are trying to predict `lpsa` (the log of the prostate-specific antigen levels) from the other features in the dataset. (Recall also that `X` is a common convention for the predictors and `y` is commonly used for the response.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your predictors and response here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to create the automation to test different models. We want to compare all of the models that use just one predictor and then compare all of the models that use two predictors and so on all the way up to eight predictors. To do this, two separate functions make sense to produce: one to generate all of the combinations of predictors and one to build the models and measure how well they explain the prostate data.\n",
    "\n",
    "This Python lab [notebook](http://www.science.smith.edu/~jcrouser/SDS293/labs/2016/lab8/) prepared by R. Jordan Crouser at Smith College has good examples you can draw from to work through (especially the `processSubset()` and `getBest()` functions). Note that there are several ways of measuring what constitutes the best model; Crouser's example uses the residual sum of squares (RSS), which measures the differences between what a model predicts and what the actual values are, squares those differences and adds them up, and it will simply things for you to go with RSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to fit an OLS model and return the model and its RSS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the model with the lowest RSS for each number of features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `getBest()` function returns a DataFrame containing the best model for a given $k$ predictors (along with how much time it took to process the models for a given value of $k$). Now create a code snippet to call `getBest()` for $k$s ranging from 1 to 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to house the results.\n",
    "# Note: This function call as written can take up to 2 to 3 minutes to run.\n",
    "\n",
    "# Start the timer to measure the total elapsed time.\n",
    "\n",
    "# Iterate over the eight possible number of features for the models.\n",
    "\n",
    "# End the time and print out the total elapsed time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have a DataFrame with the best model (and its RSS) for each value of $k$ predictors. Go ahead and take a look at the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the models DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group or Partner Discussion**\n",
    "\n",
    " - What do the columns in the `models` DataFrame represent? What pattern do you see?\n",
    "\n",
    "### The models\n",
    "\n",
    "You can actually look at the models in the DataFrame. Access the two-predictor model by using the DataFrame `loc()` method and call the `summary()` method for the model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the summary of the best two-feature model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model summary provided by the statstools API package contain far more statistical detail than we need for our prostatic modeling. Crucially, we can see that the model uses `lcavol` and `lweight`. We can also see other measures of model-goodness, including `R-squared`, `AIC`, and `BIC`, which we will discuss in greater detail below.\n",
    "\n",
    "We can also look at the attributes of the model directly, such as by accessing the `rsquared` attibute of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the R-squared score of the best two-feature model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge**\n",
    "\n",
    "With some clever coding, you can view all of the $R^2$ values. Try using the `pandas.DataFrame.apply()` method and a Python lamba function to see all eight $R^2$ scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small lambda function to print out all of the R-squared scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Intermediate Exploration\n",
    "\n",
    "Even though you it might not look like it, you now have enough information with which to determine a best model. The use of \"a\" instead of \"the\" is intentional, because we now enter a part of data science that is more art than science.\n",
    "\n",
    "There is no one measure of goodness for models. Four important ones are:\n",
    "   - **RSS**: As previously discussed, the residual squared sums provides and explanation of how much the values predicted by a model deviate from actual values; a lower value is better.\n",
    "   - **Adjusted R-squared**: This is an $R^2$ score modified for multiple regression so that it increases if an additional predictor improves the performance of the model than would be expected by chance and decreases if a new predictor detracts from model performance; a higher value is better.\n",
    "   - **AIC**: The Akaike information criterion (AIC) scores model performance and can be compared to other models with a lower score being better.\n",
    "   - **BIC**: Short for Bayesian informaion criterion, the BIC (like the AIC) scores models but also penalizes models for the number of predictors they have; a lower score is better.\n",
    "\n",
    "The [notebook](http://www.science.smith.edu/~jcrouser/SDS293/labs/2016/lab8/) prepared by R. Jordan Crouser at Smith College has a good visualization of these criteria that you can use without much modification to see how these different scores of goodness change across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a 2x2 grid so we can look at 4 plots at once.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the different criteria give us conflicting guidance on how many predictors to use in the model. RSS and adjusted $R^2$ are closely related criteria, so it is no surprise that they give similar answers; in this case, both criteria improve monotonically as the number of predictors increases. (However, it should be noted that both of those criteria can be given to overfitting.)\n",
    "\n",
    "AIC hit its minimum at five predictors while BIC had its minimum with three predictors. AIC can also be somewhat given to overfitting and with three features we can actually visualize the model, so go with the BIC recommendation for this exercise.\n",
    "\n",
    "### The three-factor model\n",
    "\n",
    "Now that we know how many features to use, open up the summary of the three-factor model from the `models` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out a summary of the three-factor model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group or Partner Discussion**\n",
    "\n",
    " - What features does this three-feature model use? Should we use any linear model with these three features, or do we need to use the specific model in the DataFrame? Why?\n",
    "\n",
    "### Visualizing the model\n",
    "\n",
    "Your next task is to create a 3D visualization of the model. Because you have three features, this should technically not be possible. However, one of the features (`svi`) is categorical, so you can actually create two graphs (one for patients with SVI and one for patients without) and plot them over each other. The Reactors Machine Learning 2 module has code that you can repurpose for your visualization here, but there are several things to be mindful of as you attempt this:\n",
    "   - Visual Studio Code [does not yet support interactive Jupyter extensions](https://github.com/microsoft/vscode-python/issues/3883), so do not include the `%matplotlib notebook` magic command or your visualization will not plot.\n",
    "   - If you use `ax = Axes3D()` instead of the `ax = fig.add_subplot(111, projection='3d')` in the original code, you will be able to supply values to the `elev` and `azim` parameters in order to rotate your graph for a clearer view.\n",
    "   - The original code generated models on the fly from within the `for` loop, but you will need to shoe-horn in the existing model (using something like `model = models.loc[3, \"model\"]`). This means that you will need to extend the `numpy.meshgrid` to extract three plots rather than two (one for each feature) and that you will need to change the dimensionality of the plots you supply to `ax.plot_surface()` to remove those artifacts left over from having to use three features rather than just two in the plotting.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model as the one arrived at through subset-selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group or Partner Discussion**\n",
    "\n",
    " - That was a lot of work to produce, but visualzations are often the best means of exploring data. What patterns emerge in the data from this visualzation of the data overlaid on the model predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Individual Exploration\n",
    "\n",
    "The current model that we have built up in the course of this workshop seems to intuitively explain the relationship between prostate weight, cancer volume, seminal vesicle invasion, and prostate-specific antigen. But how is it at prediction? Are there other ways of selecting our model?\n",
    "\n",
    "Let's tackle the second question first. Thus far we have used something of a brute-force method for building our model: we looked at all of the various combinations of features and selected the model with the best performance. And with eight independent variables this was feasible. But the number of combinations increases exponentially with the number of features we want to compare, and the methods we have used thus far of subset selection could quickly become too slow to use.\n",
    "\n",
    "On way of dealing with this challenge is to avoid it altogether. Rather than evaluate every possible subset of features, another way forward is to take a page from principal-component analysis (PCA) to reduce the number of features and then build a model in the reduced feature space. (For more information on using PCA hands-on in life sciences, see the Reactors Machine Learning 1 workshop.) This is the idea behind principal-component regression (PCR).\n",
    "\n",
    "PCR handles feature-reduction in much the same way that PCA handles dimension-reduction: it looks for those components of the data (clusters of data, and their dimensions) that contain the most information, and then works from there. Put another way, PCR uses PCA to figure out which combination of features provides the best model for the data.\n",
    "\n",
    "Your task is going to be to work through PCR on the same prostate dataset you have worked with so far in this workshop. To get started, re-define your `X` to be all of the variable from the prostate dataset except `lpsa` and re-define `y` to be `lpsa` (just in case anything has changed in the course of your work).\n",
    "\n",
    "We are also interested in evaluating the predictive power of our model (rather than the just the explanatory power of a model as we did above): what combination of features in prostate patients helps predict their prostate-specific antigen levels? To do this, you will need to split your data into training and test subsets; the [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) helper function in `sklearn.model_selection` is particularly useful for this. (It is up to you to determine what proportion of the data to use for training and how much to reserve for testing.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your predictors and response.\n",
    "\n",
    "# Split the predictors and response into training and test sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal-component regression\n",
    "\n",
    "Now for actually performing PGR. PGR is a two-step process. First, perform PCA on your independent variables in the training data (but not the entirety of the training data -- leave the independent variable out of this!). Second, build a models models of of the transformed training data using differing number of principal components to see which one gives you the greatest accuracy.\n",
    "\n",
    "Because you will be doing the PCR steps repeatedly, it is advisable that you define a function to do this for you. This [blog post](https://nirpyresearch.com/principal-component-regression-python/) has good function in it that you can tailor to your needs. (Note that one alteration you will need to make to it is to make is accommodate your split training and test data; you will want to be very mindful of where you use which subset of data within the function.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your PCR function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "You might have noticed that the function in the blog post linked to above used two kinds of prediction: the vanilla type that you we have encountered before and cross validation.\n",
    "\n",
    "One phenomenon you might have noticed as you have experimented with ML in Rectors workshops or elsewhere is that you can get different results if you change aspects of your training/test split of the data (either by changing the amount reserved for testing or changing the random seed). These alterations change what collection of data is used to train your models and evaluate them. So how can you know if you have made a split in your data that is just unusually lucky (or unfortunate)? In other words, how can you know that you are splitting you data to train the most robust model you can?\n",
    "\n",
    "You could try running model using a variety of splits in your data. While not a bad idea, you might not be sufficiently systematic in trying different split. A more thorough method is one developed by statisticians: cross-validation. The idea behind cross-validation is to split your training data into $k$ smaller sets and then iterate over those smaller sets using $k-1$ of them one of them as a mini training sets and one of them as a mini test set. By doing so, you can reasonably hope to get a better-rounded sense of the true performance of you model on a variety of data without having to sacrifice data for validating your model. (Scikit-learn has a very good [page](https://scikit-learn.org/stable/modules/cross_validation.html) on cross-validation that goes into far more detail.)\n",
    "\n",
    "### Mean squared error\n",
    "\n",
    "The blog post also uses a different metric for model accuracy that we have not yet used in this workshop: mean squared error (MSE). MSE is basically RSS divided by the number of observations the dataset (technically, it is RSS divided by the number of degrees of freedom of the residuals, but that is just the number of observations - 2) and is widely used in biosciences (and all sciences and statistics more broadly). \n",
    "\n",
    "### PGR in action\n",
    "\n",
    "Now that you have created your function, use a `for` loop to call it for all of the possible number of principal components (1-8) and compare the conventional $R^2$ values, those computed using cross-validation, the MSE values, and the values of MSE acquired through cross-validation. (Consider storing your results in a DataFrame to make the results easier to read and analyze.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to house the PCR results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we want to choose is the lowest MSE value computed using cross-validation, which points to the model fitted with the first three principal components. This model also has the highest $R^2$ score achieved using cross-validation. What is noteworthy is that this model disagrees with that selected by looking at the convetional MSE and $R^2$ score, which is to use the model fitted with the first two principal components (at least when `test_size=0.6` and `random_state=42` used in `train_test_split`).\n",
    "\n",
    "**Group or Partner Discussion**\n",
    "\n",
    " - What does this divergence in model selection mean?\n",
    "\n",
    "Let's compare the $R^2$ score and MSE of our model selected through PCR to that which produced through subset selection. (The OLS model that we created earlier in this workshop has `rsquared` and `mse_resid` attributes that you can print out.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the R-squared and residual MSE for the subset-selection model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group or Partner Discussion**\n",
    "\n",
    " - What does it mean that our model produced using PCR and cross-validation has a slighly higher MSE and a much lower $R^2$ score than the subset-selection model that didn't split its data at all?\n",
    "\n",
    "### What exactly is our PCR model?\n",
    "\n",
    "Notice that the $R^2$ score for the models fitted using cross-validation are uniformly lower than our model generated through subset-selection and *only get lower as we add more principal components*. To understand what might cause this, it's helpful to step back and examine what it is that we have created using PCR. Start by printing out the coefficients of the model (the $\\beta$s in the model for all of the math fans out there). (It is probably easiest just to re-fit the model using the first three principal components and then print out the `coef_` attribute for the model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PCA object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In re-fitting the model, you also re-fitted the PCA object. Print out its `components_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out all of the PCA components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can help to just print out the first three principal components. (The scikit-learn PCA algorithm automatically sorts the components in order of explained variance; in other words, in order of importance to our PCR model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the components we are interested in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our careful process of deduction has led us to a model of the multiplies these three vectors by the three coefficients we extracted three steps above. In practice, we would use this model by centering and transforming new data (that is, subtract the mean of each feature from a new data point and divide it by each feature's standard deviation). The new data (even after transformation and centering) is in the form of a 8x1 matrix (one observation of the eight variables). We would then multiply that matrix by our 3x8 matrix of the principal components, which would yield a 1x3 matrix of that we would then multiply by the 3x1 matrix of model coefficients to get our prediciton.\n",
    "\n",
    "That's how we would use the model, but what does it *mean*?\n",
    "\n",
    "Deriving meaning from principal components can be tough because they are not tied to any one feature or even subset of features: they are derived from all of the features. (The eight numbers in each row of the matrix we printed in the step above is represent the component of that matrix in the eight dimensions of the feature space.) But tough is not the same as impossible, and it can be helpful to understanding to translate our PCR model back into terms of our familiar features.\n",
    "\n",
    "To do this, we have to reason through what the matrix multiplication is doing. One way of doing that might be to take the averages of our features and feed those into our model. But because we build this model on centered data, each of those variables would end up being 0. (Indeed, we leave it as an exercise to mathematically inclined students to satisfy themselves that, because using centered means in this model means that we will end up multiplying our coefficients by 0 that `regr.intercept_` in the model is actually just the mean of the response, `lpsa`.)\n",
    "\n",
    "But what happens if we multiple the PCA components and model coefficients without any data it in? Use the `numpy.dot` universal function to do this. (You will likely find it helpful to display the output in a DataFrame with the index changed to the feature names to aid with interpretation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store the results.\n",
    "\n",
    "# Create new column for the model coefficients.\n",
    "\n",
    "# Change the index names to match the features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DataFrame of coefficients shows what we would actually multiply the (centered and transformed) feature values by in order to make predictions. Our predictive model involves all of the features, but some more than others. We can highlight the relative importance of featues in this model by normalizing the coefficients using the scikit-learn `MinMaxScaler` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the values from the DataFrame.\n",
    "\n",
    "# Define the scaler.\n",
    "# The values variable defined above aids in specifying the minimum end of the transformation.\n",
    "\n",
    "# Apply the transformation to the values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important variable in our PCR model is `lcavol`, with `lweight` being 88% as important and `svi` being 59% as important. So far this is similar to the model we created using subset selection, but not identical. Different from our subset-selection model, `lcp` plays a sizeable role (42% as important to the model as `lcavol`) while `gleason` and `pgg45` both have a non-neligiable *negative* correlation in the model. It is also easier to see with the normalized coefficients that `age` and `lbph` determine very little in this model.\n",
    "\n",
    "So our PCR model can be interpretted in terms of the features from the dataset. To return to the earlier rhetorical question of why PCR generates a less useful model with additional principal components: the model because overfitted because each feature plays a more important part in the model.\n",
    "\n",
    "Cross-validation indicates that the PCR-derived model will yield a pretty good prediction of `lpsa` with new data. But it is harder to explain just how the model arrived at its prediction with the PCR model than with our subset-selection model. Put another way, PCR creates a model that is, one its surface, something of a black box. It's not totally impenetrable, but because the PCR model uses principal components rather than features, it is harder to tease meaning out of it than the relatively more straightforward model produced through subset-selection.\n",
    "\n",
    "All of this is a way of saying that the kind of models that we build in the biosciences matters for reasons beyond accuracy of completion. When practicing data science in this field, it is vital to always keep in the forefront of our thinking what the purpose of the model is. If it is to explore and explain data, a model built using features is probably more approrpriate. If the goal is to produce good predictions, then we might care less about the transparency of the model and PCR with cross-validation might be the preferred route. Regardless of the purpose, however, it is important to work early on with domain experts and final consumers of the models to see what they need and to think early and often about how your models will help them meet their needs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}