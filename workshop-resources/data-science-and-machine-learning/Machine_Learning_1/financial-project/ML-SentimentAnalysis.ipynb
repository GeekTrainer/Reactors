{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning With Text : Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first cut at a series of exercises to conduct sentiment analysis for the Microsoft Reactor Machine Learning initiative. Machine learning does not just have to be about numbers. We can turn text into numbers and look at things like document similarity, clusters, etc. We can also engage Natural Language Processing (NLP) tools to perform sentiment analysis, named entity recognition and more.\n",
    "\n",
    "Consider the explosion of data we face and then realize that we have similar issues with documents in the form of news stories or magazine articles. You either need to read very fast (or constantly) or rely on some reporting source to tell you what is going on. It is clear that not every news story is unbiased and sometimes there may be positive or negative spin on a story, so how do you look behind the scenes to see what is going on?\n",
    "\n",
    "We can increasingly rely on machine learning systems to help us process documents and extract information and other value from them. If you work in a particular industry, you could start to see what is being said about your industry. If you have an investment portfolio, you could imagine letting software read the news and draw your attention to positive or negative swings in reporting. If you work for an organization, you could get a sense of what people were saying about you on social media.\n",
    "\n",
    "In our exercise, we will use an RSS feed of the news headlines from Microsoft Money as a source of data. As time passes, you could store and assess what is the general sentiment of the news? Who is being mentioned most often? Are positive and negative things being said about these individuals, organizations or places?\n",
    "\n",
    "PLEASE NOTE: THE RESULTS OF THIS EXERCISE WILL CHANGE OVER TIME AND DO NOT CONSTITUTE LEGAL OR FINANCIAL ADVICE. PLEASE CONSULT WITH YOUR OWN EXPERTS BEFORE MAKING ANY DECISIONS ON WHAT YOU READ IN THE NEWS OR SEE HERE.\n",
    "\n",
    "You will need to install BeautifulSoup, nltk and feedparser.This is the first cut at a series of exercises to conduct sentiment analysis for the Microsoft Reactor Machine Learning initiative. Machine learning does not just have to be about numbers. We can turn text into numbers and look at things like document similarity, clusters, etc. We can also engage Natural Language Processing (NLP) tools to perform sentiment analysis, named entity recognition and more.\n",
    "\n",
    "Consider the explosion of data we face and then realize that we have similar issues with documents in the form of news stories or magazine articles. You either need to read very fast (or constantly) or rely on some reporting source to tell you what is going on. It is clear that not every news story is unbiased and sometimes there may be positive or negative spin on a story, so how do you look behind the scenes to see what is going on?\n",
    "\n",
    "We can increasingly rely on machine learning systems to help us process documents and extract information and other value from them. If you work in a particular industry, you could start to see what is being said about your industry. If you have an investment portfolio, you could imagine letting software read the news and draw your attention to positive or negative swings in reporting. If you work for an organization, you could get a sense of what people were saying about you on social media.\n",
    "\n",
    "In our exercise, we will use an RSS feed of the news headlines from Microsoft Money as a source of data. As time passes, you could store and assess what is the general sentiment of the news? Who is being mentioned most often? Are positive and negative things being said about these individuals, organizations or places?\n",
    "\n",
    "PLEASE NOTE: THE RESULTS OF THIS EXERCISE WILL CHANGE OVER TIME AND DO NOT CONSTITUTE LEGAL OR FINANCIAL ADVICE. PLEASE CONSULT WITH YOUR OWN EXPERTS BEFORE MAKING ANY DECISIONS ON WHAT YOU READ IN THE NEWS OR SEE HERE.\n",
    "\n",
    "You will need to install BeautifulSoup, nltk and feedparser if they are not installed.\n",
    "\n",
    "To install them, you can type:\n",
    "\n",
    "```pip install bs4 nltk feedparser```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: bs4 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (0.0.1)\nRequirement already satisfied: nltk in c:\\users\\sarah\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (3.5)\nCollecting feedparserpip\n  ERROR: Could not find a version that satisfies the requirement feedparserpip (from versions: none)\nERROR: No matching distribution found for feedparserpip\nWARNING: You are using pip version 19.2.3, however version 20.1.1 is available.\nYou should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
    }
   ],
   "source": [
    "!pip install bs4 nltk feedparserpip install bs4 nltk feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Sarah\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Sarah\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     C:\\Users\\Sarah\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import collections\n",
    "import string\n",
    "import feedparser\n",
    "import requests\n",
    "import hashlib\n",
    "import tempfile\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "import numpy as np\n",
    "\n",
    "# We need to load the Stopwords and the lexicons that the VADER algorithm uses to assess sentiment.\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first method is going to create a directory for us to cache our results in so we do not have to keep downloading them over and over. We will use the OS definition of where temporary directories should go, and then we will create a subdirectory for our purposes called `msft-reactor-ml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTempDirIfNecessary() :\n",
    "    tempDir = tempfile.gettempdir()\n",
    "    path = os.path.join(tempDir, \"msft-reactor-ml\")\n",
    "\n",
    "    if(not os.path.exists(path)) :\n",
    "        os.mkdir(path)\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because during development, you might want to run your code many times over fairly large collections of documents, you don't want to be a bad citizen and pound backend servers with requests. So, we add the following function to store files after downloading them. We hash the URL to a consistent name so if we go looking for it in the future, we can just grab the local copy.\n",
    "\n",
    "*Note*: This approach is only triggering off the location identity for the documents, not the contents. As a homework assignment, you might improve this function to use cache-controls from the server or check to see if the timestamp of the file is older than a particular age. If it is, you might ditch the cached copy to fetch a newer one. That's mostly going to be useful for URL results that change over time like RSS feeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchUrlIfNecessary(url) :\n",
    "    # Initialize the temporary directory if we need to\n",
    "    path = createTempDirIfNecessary()\n",
    "\n",
    "    # Hash a normalized version of the URL using a SHA-256\n",
    "    # hashing function.\n",
    "    hashed = hashlib.sha256(url.encode()).hexdigest()\n",
    "\n",
    "    # The raw file will be stored as <HASH>.in.txt to differentiate\n",
    "    # it from a cleaned up or processed version as we will see later.\n",
    "    file = os.path.join(path, f\"{hashed}.in.txt\")\n",
    "\n",
    "    # Once we know what the associated file with the URL should be called\n",
    "    # (via its hash, not URL), we will see if it exists. If it doesn't, we\n",
    "    # will store it.\n",
    "    if(not os.path.exists(file)) :\n",
    "        response = requests.get(url)\n",
    "        with open(file, mode='wb') as localfile:\n",
    "            localfile.write(response.content)\n",
    "\n",
    "    # Whether the file existed before or not, we return the hashed name of \n",
    "    # the file so analytical code can just open it up as need be.\n",
    "    return file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method was inspired by the [StackOverflow referenced below](https://stackoverflow.com/questions/30565404/remove-all-style-scripts-and-html-tags-from-an-html-page/30565420). When you are extracting text from structured documents such as HTML, there are several elements that will just be noise such as JavaScript script elements, stylesheet elements, etc. A given source may require additional handling so for any particular site that you want to scrape from, you may need to parameterize the elements to remove.\n",
    "\n",
    "For convenience, we add the ability to specify a list of DOM element classes and ids to remove as well. You will see an example of that being used below. To start off with, however, you can just pass in the HTML file as a string and BeautifulSoup will do most of the hard work for us. \n",
    "\n",
    "*Note:* As a homework assignment, refactor this method to accept the HTML elements as a default parameter so they can be overridden as needed by client code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/30565404/remove-all-style-scripts-and-html-tags-from-an-html-page/30565420\n",
    "\n",
    "def cleanMe(html, class_filters=[], id_filters=[]):\n",
    "    soup = BeautifulSoup(html, \"html.parser\") # create a new bs4 object from the html data loaded\n",
    "    \n",
    "    # Remove all javascript and stylesheet code\n",
    "    for script in soup([\"script\", \"style\", \"a\", \"li\", \"noscript\", \"span\", \"meta\"]): \n",
    "        script.extract()\n",
    "\n",
    "    # Remove any elements that have any of the specified class identifiers.\n",
    "    # There could be several instances.\n",
    "    for c in class_filters:\n",
    "        elems = soup.find_all(class_=c)\n",
    "        for elem in elems:\n",
    "            elem.decompose()\n",
    "\n",
    "    # Remove any elements that have any of the specified element identifiers.\n",
    "    # There should only be one instance per id.\n",
    "    for i in id_filters:\n",
    "        elem = soup.find(id=i).decompose()\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method retrieves the local copy of a file associated with a URL and then processes it prior to analysis.\n",
    "\n",
    "*INSTRUCTOR NOTE*: If you want the students to spend some time, have them pick a new data source and figure out which id filters and class filters should be removed to improve the quality of the results. The values below are useful to remove from the BBC RSS feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchAndProcessUrlIfNecessary(url) :\n",
    "    entryFile = fetchUrlIfNecessary(url)\n",
    "\n",
    "    # The value returned from the fetchUrlIfNecessary function will\n",
    "    # be <HASH>.in.txt so we grab the <HASH> part and indicate that\n",
    "    # this is the processed version.\n",
    "    processedFile = f\"{entryFile[:-7]}.out.txt\"\n",
    "\n",
    "    # If the file doesn't exist, we will process the raw file.\n",
    "    if(not os.path.exists(processedFile)) :\n",
    "        with open (entryFile, \"r\" ) as myfile:\n",
    "            data = myfile.read()\n",
    "            data = cleanMe(data,\n",
    "                id_filters=[\n",
    "                  'orb-header', \n",
    "                  'core-navigation'\n",
    "                ],\n",
    "                class_filters=[\n",
    "                  'off-screen', \n",
    "                  'tags-container', \n",
    "                  'faux-block-link',\n",
    "                  'distinct-component-group',\n",
    "                  'orb-footer', \n",
    "                  'share', \n",
    "                  'column--secondary', \n",
    "                  'more-on-this-story',\n",
    "                  'story-body__h1',\n",
    "                  'player-with-placeholder',\n",
    "                  'vxp-media-player-component',\n",
    "                  'vxp-related-content-component'\n",
    "                ])\n",
    "\n",
    "            # We will store the result and close\n",
    "            # up our files.\n",
    "            outfile = open(processedFile, \"w\")\n",
    "            outfile.write(data)\n",
    "            outfile.close()\n",
    "            myfile.close()\n",
    "    else:\n",
    "        # If the processed file exists, we will just return\n",
    "        # the results for further analysis.\n",
    "        with open (processedFile, \"r\" ) as myfile:\n",
    "            data = myfile.read()\n",
    "            myfile.close()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is simply a place to experiment with the fetchAndProcessUrlIfNecessary() function. As you experiment with the previous functions, you can see what the resulting document looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Coronavirus: China's plan to test everyone in Wuhan - BBC News\nChina has completed a mass testing programme in Wuhan, the city where the Covid-19 pandemic began.The authorities had pledged to test the entire city over a 10-day period after a cluster of new infections arose.We've looked at the original plan, and what was achieved.What was the target?Wuhan has an estimated population of 11 million people, so aiming to test everyone in 10 days would have been an ambitious target. But those already tested in the seven days prior to mass testing starting, as well as any children under six years of age, were excluded from the programme.\nThe total number of tests needed may have been reduced further given that some residents who left Wuhan before the lockdown in January may well not have returned.However, we don't have an exact number for this.\nAlso,\nthe timeframe has shifted somewhat since the initial announcement of a 10-day programme of testing, which was made on 12 May. The Wuhan authorities later said different districts within the city would be starting at different times.\n\"Each district finishes its tests within 10 days from the date it started them,\" the Wuhan Centre for Disease Control said, which effectively extended the deadline beyond the original pledge.How many have been tested?All the data we have comes from official sources in Wuhan, and there's no independent verification for the numbers.As of 1 June, a total of 9.9 million people had been tested, according to the city health authorities.They said this marks the end of the mass testing programme.They also said that if you include one million people tested in the seven days before the mass programme began in their area (and who didn't need retesting), that's a total of 10.9 million people tested out of the population of 11 million.It's taken longer to test everyone than the 10-day period that was promised when the plan was first announced.However, they did manage to collect as many as nine million test samples after 10 days, so nearly the entire population.\nThat was largely achieved through a significant boost to daily testing capacity.Before they began the mass testing, there were about 60 centres across the city, with an overall maximum capacity of 100,000 tests a day, according to the official Hubei Daily newspaper.By 1 June, the authorities said they had 249 testing centres operating.They also mobilised teams to test old people, and the disabled or vulnerable in their own homes.There were more than 1,450 testing staff involved.The other way they sped up the process was to use a batch testing method, which groups individual test samples together.Reports suggest they used batches of between five and 10 samples in Wuhan, only carrying out individual tests if a batch proved positive.\nAnd as many as 25% of all tests were done using this method. This is an efficient way to test large numbers of people where infection levels are low, as most batches would produce negative results.And it appears to have worked in Wuhan because 97% of local communities across the city reported no positive tests, according to the official data.The authorities said they had found just 300 positive cases (all without symptoms) out of all the tests done, and traced a further 1,174 close contacts of these people.\n"
    }
   ],
   "source": [
    "parsedFile = fetchAndProcessUrlIfNecessary(\"https://www.bbc.co.uk/news/world-asia-china-52651651\")\n",
    "print(parsedFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is intended to be modified during an exercise. Without the clean up steps, the results will suffer. Once you try it a few times without the clean up steps, you should remove some of the comments and lowercase the words,\n",
    "remove punctuation and stopwords, stem the tokens, etc. and see how the analysis improves down below.\n",
    "\n",
    "*INSTRUCTOR NOTE:* Make sure the students know what they are doing and which of the comments need to go together. It's probably more interesting and useful if they don't uncomment everything all at once, but for example, you need to uncomment the stop_words set creation AND the if clause to remove the stopwords. Remind the students to pass in True for the filterStopWords parameter to activate that behavior in the tokenization process. It's left to you which lines to start commented out depending on time and course configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenizeText(text, filterStopWords=False) :\n",
    "   #stem = nltk.stem.SnowballStemmer('english') \n",
    "   text = text.lower()\n",
    "   stop_words = set(stopwords.words('english'))\n",
    "\n",
    "   for token in nltk.word_tokenize(text):\n",
    "       if token in string.punctuation: continue\n",
    "       if filterStopWords and token in stop_words: continue\n",
    "       yield token # stem.stem(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is intended as an exercise if you have time. We want you to count how many of each word shows up in a text. You will need to tokenize the text with the previous method. As you make changes to the body of the previous method, you should see different results. What happens if you don't filter out the stopwords? What if you don't lower case the text?\n",
    "\n",
    "*INSTRUCTOR NOTE*: Leave the function definition as it is but maybe remove the body. The students should be told to tokenize the text with the method above and iterate over the results. If they don't filter stop words, then common words will show up as the most common words. If they don't lowercase the text above, case differences will show up as different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordFrequency(text, filterStopWords=False) :\n",
    "   dict = collections.Counter()\n",
    "   for token in tokenizeText(text, filterStopWords):\n",
    "       dict[token] += 1\n",
    "   return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is intended as an exercise if you have the time. We want you to tokenize the text as sentences and then analyze each one for its sentiment. Return a list with each sentence and its scores. Keep in mind you can add the sentence to the results from the NLTK Sentiment analyzer.\n",
    "\n",
    "*INSTRUCTOR NOTE:* Leave the function definition as is but maybe remove the body. The students should be told to tokenize the text as sentences run them through the NLTK SentimentIntensityAnalyzer. The return value should be a list of dicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeSentiment(text):\n",
    "    results = []\n",
    "\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    for sentence in sentences:\n",
    "        scores = sid.polarity_scores(sentence)\n",
    "        scores['text'] = text\n",
    "        results.append(scores)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method converts a list of dicts into a DataFrame. It's left as a utility for the students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "def createDataFrameFromResults(results) :\n",
    "    df = pd.DataFrame.from_records(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is a convenience to change the source of our documents. We had intended to use MSN but that feed stopped working inexplicably so we have the BBC's World RSS feed as our default. There is also a feed of Japanese documents if you want to experiment with languages other than English. Keep in mind that different sources of documents may require different pre-processing to achieve better results. The parsing code may need to change slightly, the elements you need to discard before extracting the text may change slightly, but you have several options including class and id elements to filter in the tokenizeText methods above. Feel free to experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchFeed() :\n",
    "    #feedURL = \"http://rss.msn.com/en-us/money?feedoutput=rss\"\n",
    "    #feedURL = \"http://rss.asahi.com/rss/asahi/newsheadlines.rdf\"\n",
    "    feedURL = \"http://feeds.bbci.co.uk/news/world/rss.xml\"\n",
    "    feedFile = fetchUrlIfNecessary(feedURL)\n",
    "    feed = feedparser.parse(f\"file:///{feedFile}\")\n",
    "    return feed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "*INSTRUCTOR NOTE*: This is the first main exercise. It is intended to familiarize the students with the idea of processing a corpus, cleaning/pre-processsing the text, accumulating various results and then summarizing them. There are a variety of approaches you can take depending upon the time available and the background of the students.\n",
    "\n",
    "* You can simply walk through the solution and highlight the steps. Dive into the function definitions and explain where they can experiment and expect to see different results.\n",
    "* You can remove the definitions of the main functions that are called from this function and ask the students to fill one or more of them in. This is a way to constrain the exercise for scope and duration. The word frequency and sentiment analysis functions are the ones that are thought to be challenging but tractable.\n",
    "* In a longer course configuration or with advanced students you can have the students fill in the whole function. It's probably worthwhile to include at least pseudocode of the flow. Otherwise that might be too much to tackle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fetching: https://www.bbc.co.uk/news/technology-53425822\n-------------\nMajor US Twitter accounts hacked in Bitcoin scam:, Comp: -0.7506, Pos: 0.0 , Neu: 0.484,  Neg: 0.516\nneg         0.067839\nneu         0.870258\npos         0.062000\ncompound   -0.099135\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-us-canada-53426285\n-------------\nBrad Parscale replaced as Trump's campaign manager:, Comp: 0.0, Pos: 0.0 , Neu: 1.0,  Neg: 0.0\nneg         0.045667\nneu         0.945333\npos         0.009000\ncompound   -0.474933\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/business-53399999\n-------------\nCoronavirus: Chinese economy bounces back into growth:, Comp: 0.3818, Pos: 0.302 , Neu: 0.698,  Neg: 0.0\nneg         0.011000\nneu         0.920333\npos         0.068667\ncompound    0.410533\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-us-canada-53423927\n-------------\nCoronavirus: US disease chief Dr Anthony Fauci calls White House attacks 'bizarre':, Comp: -0.4404, Pos: 0.0 , Neu: 0.791,  Neg: 0.209\nneg         0.067696\nneu         0.833870\npos         0.098391\ncompound   -0.081165\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-53420409\n-------------\nChild vaccinations fall sharply amid pandemic, UN says:, Comp: 0.0, Pos: 0.0 , Neu: 1.0,  Neg: 0.0\nneg         0.132000\nneu         0.829909\npos         0.037909\ncompound   -0.481600\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/technology-53412678\n-------------\nHuawei: China attacks UK's 'groundless' ban of 5G kit:, Comp: -0.7579, Pos: 0.0 , Neu: 0.519,  Neg: 0.481\nneg         0.101308\nneu         0.823115\npos         0.075615\ncompound   -0.068342\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-us-canada-53425238\n-------------\nTrump weakens environmental law to speed up infrastructure projects:, Comp: -0.3182, Pos: 0.0 , Neu: 0.777,  Neg: 0.223\nneg         0.047824\nneu         0.896647\npos         0.055588\ncompound   -0.034176\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-middle-east-53417228\n-------------\nIran judiciary halts protesters' executions after social media storm:, Comp: -0.2263, Pos: 0.0 , Neu: 0.808,  Neg: 0.192\nneg         0.173667\nneu         0.767429\npos         0.058810\ncompound   -0.372167\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-europe-53415693\n-------------\nAzerbaijan protesters demand war after Armenia clashes:, Comp: -0.743, Pos: 0.0 , Neu: 0.354,  Neg: 0.646\nneg         0.155250\nneu         0.826250\npos         0.018500\ncompound   -0.358505\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-middle-east-53417227\n-------------\nBushehr port: Seven ships ablaze in latest Iran mystery fires:, Comp: 0.0, Pos: 0.0 , Neu: 1.0,  Neg: 0.0\nneg         0.13820\nneu         0.85700\npos         0.00480\ncompound   -0.55912\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-europe-53424074\n-------------\nCoronavirus: Ireland puts brakes on easing lockdown amid 'real concern':, Comp: 0.25, Pos: 0.182 , Neu: 0.818,  Neg: 0.0\nneg         0.074818\nneu         0.856364\npos         0.068909\ncompound    0.058436\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-africa-53416277\n-------------\nRiver Nile dam: Reservoir filling up, Ethiopia confirms:, Comp: 0.0, Pos: 0.0 , Neu: 1.0,  Neg: 0.0\nneg         0.058762\nneu         0.887952\npos         0.053286\ncompound   -0.078281\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-53424726\n-------------\nSeven countries with big (and small) population problems:, Comp: -0.4019, Pos: 0.0 , Neu: 0.722,  Neg: 0.278\nneg         0.043148\nneu         0.912778\npos         0.044111\ncompound    0.056326\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-53353748\n-------------\nCoronavirus: 'How we’re surviving a second virus lockdown':, Comp: 0.296, Pos: 0.268 , Neu: 0.732,  Neg: 0.0\nneg         0.048280\nneu         0.829520\npos         0.122180\ncompound    0.173228\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-asia-india-53361538\n-------------\nIndia coronavirus: Kuwait's new expat bill has Indians worried:, Comp: -0.296, Pos: 0.0 , Neu: 0.784,  Neg: 0.216\nneg         0.048297\nneu         0.913378\npos         0.038324\ncompound   -0.029811\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-europe-53415780\n-------------\nRussia’s young liberals confronted by MeToo moment:, Comp: -0.2023, Pos: 0.0 , Neu: 0.769,  Neg: 0.231\n"
    },
    {
     "output_type": "error",
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 122457: character maps to <undefined>",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-d6752f6f9b01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# the source servers.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mentryFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetchAndProcessUrlIfNecessary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# Let's count the word frequencies for the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-0cc6488359b6>\u001b[0m in \u001b[0;36mfetchAndProcessUrlIfNecessary\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessedFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mentryFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             data = cleanMe(data,\n\u001b[0;32m     14\u001b[0m                 id_filters=[\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 122457: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# Retrieve a parsed version of our data source\n",
    "feed = fetchFeed()\n",
    "\n",
    "# Initialize a Counter to keep track of all the words\n",
    "# in our entire corpus (the RSS feed).\n",
    "corpusFreq = collections.Counter()\n",
    "\n",
    "# We iterate over each feed entry and will analyze it\n",
    "# in isolation. We will also aggregate word counts \n",
    "# across all of the documents.\n",
    "\n",
    "for post in feed.entries :\n",
    "    print(f\"Fetching: {post.link}\")\n",
    "\n",
    "    # We want to analyze the sentiment of the titles as\n",
    "    # distinct from the body of the articles. Are headlines\n",
    "    # spun to be more positive or negative?\n",
    "\n",
    "    results = analyzeSentiment(post.title)\n",
    "    print('-------------')\n",
    "    comp = results[0]['compound']\n",
    "    pos = results[0]['pos']\n",
    "    neu = results[0]['neu']\n",
    "    neg = results[0]['neg']\n",
    "\n",
    "    print(f\"{post.title}:, Comp: {comp}, Pos: {pos} , Neu: {neu},  Neg: {neg}\")\n",
    "\n",
    "    # We fetch a processed, cleaned up version of each entry. It's either\n",
    "    # locally cached or will be as part of this request so we don't hammer\n",
    "    # the source servers.\n",
    "\n",
    "    entryFile = fetchAndProcessUrlIfNecessary(post.link)\n",
    "\n",
    "    # Let's count the word frequencies for the file\n",
    "    docFreq = wordFrequency(entryFile, True)\n",
    "\n",
    "    # Update the corpus Counter with this document's contributions\n",
    "    corpusFreq.update(docFreq)\n",
    "\n",
    "    # Analyze the Sentiment of the article.\n",
    "    results = analyzeSentiment(entryFile)\n",
    "\n",
    "    # This is a somewhat gratuitous use of Pandas, but we just wanted to demonstrate\n",
    "    # how to connect these NLP techniques to tools you have learned to use in other\n",
    "    # Machine Learning courses. Here we simply convert the per sentence sentiment results\n",
    "    # into a Pandas DataFrame and then find the average score for the document (over all of\n",
    "    # the sentences).\n",
    "    df = createDataFrameFromResults(results)\n",
    "    print(df.mean())\n",
    "    print()\n",
    "\n",
    "# After we accumulate the results for each of the documents in the feed, we will\n",
    "# print out the 50 most common words across all of the documents in the feed.\n",
    "# Keep in mind that the quality of the results can be improved by removing common words,\n",
    "# lowercasing the tokens, removing punctuation, stop words, noisy HTML elements and \n",
    "# more. These results won't be perfect. As a homework item, try to improve the quality of\n",
    "# the word summaries by doing a better job cleaning up the source data.\n",
    "print(corpusFreq.most_common(50))\n",
    "print(\"Finished Processing Feed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "*INSTRUCTOR NOTE:* This is the next prominent exercise. The idea here is to do some analysis of the text documents using conventional tools such as the TfidVectorizer from Sci-Kit Learn. This is a remarkably straightforward analysis that doesn't involve the kind of math some students would learn in high school. The Text Vectorization process is lossy, however, so there are limits to the quality of the results. Still, this has been a useful advancement in the history of NLP. For the exercise, you can either just walk through it in the interest of time, or pick some piece of the process to create. The math to generate the correlation table is probably not something we can expect everyone to be familiar with, however, so you are strongly encouraged to at least give them that part of the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fetching: https://www.bbc.co.uk/news/technology-53425822\nFetching: https://www.bbc.co.uk/news/world-us-canada-53426285\nFetching: https://www.bbc.co.uk/news/business-53399999\nFetching: https://www.bbc.co.uk/news/world-us-canada-53423927\nFetching: https://www.bbc.co.uk/news/world-53420409\nFetching: https://www.bbc.co.uk/news/technology-53412678\nFetching: https://www.bbc.co.uk/news/world-us-canada-53425238\nFetching: https://www.bbc.co.uk/news/world-middle-east-53417228\nFetching: https://www.bbc.co.uk/news/world-europe-53415693\nFetching: https://www.bbc.co.uk/news/world-middle-east-53417227\nFetching: https://www.bbc.co.uk/news/world-europe-53424074\nFetching: https://www.bbc.co.uk/news/world-africa-53416277\nFetching: https://www.bbc.co.uk/news/world-53424726\nFetching: https://www.bbc.co.uk/news/world-53353748\nFetching: https://www.bbc.co.uk/news/world-asia-india-53361538\nFetching: https://www.bbc.co.uk/news/world-europe-53415780\n"
    },
    {
     "output_type": "error",
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 122457: character maps to <undefined>",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-dc91084f98d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Fetching: {post.link}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mentryFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetchAndProcessUrlIfNecessary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mdocuments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentryFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mheadlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-0cc6488359b6>\u001b[0m in \u001b[0;36mfetchAndProcessUrlIfNecessary\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessedFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mentryFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             data = cleanMe(data,\n\u001b[0;32m     14\u001b[0m                 id_filters=[\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 122457: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "\n",
    "# Retrieve a parsed version of our data source\n",
    "feed = fetchFeed()\n",
    "\n",
    "# We are going to accumulate the processed forms of the input \n",
    "# documents and their headlines.\n",
    "documents = []\n",
    "headlines = []\n",
    "\n",
    "# Iterate over all of the input documents to grab the document\n",
    "# text and the associated headlines.\n",
    "for post in feed.entries :\n",
    "    print(f\"Fetching: {post.link}\")\n",
    "\n",
    "    entryFile = fetchAndProcessUrlIfNecessary(post.link)\n",
    "    documents.append(''.join(entryFile))\n",
    "    headlines.append(post.title)\n",
    "\n",
    "# The TF-IDF Vectorizer class is from Sci-Kit Learn and allows us\n",
    "# to vectorize the corpus based upon this common NLP tool:\n",
    "# https://en.wikipedia.org/wiki/Tf–idf\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# We transform the documents into a collection of vectors and \n",
    "# compare them to each other. An explanation of what is going\n",
    "# on can be found here: https://medium.com/@odysseus1337/document-class-comparison-with-tf-idf-python-1b4860b9345b\n",
    "vecs = tfidf.fit_transform(documents)\n",
    "matrix = ((vecs * vecs.T).A)\n",
    "\n",
    "# We convert the first row into a data frame. Each value represents the first document with its\n",
    "# correlation against the other documents based upon the TF-IDF vectorization. We index the DataFrame\n",
    "# by the corresponding headlines so its easier to see which articles are more alike which other ones.\n",
    "df = pd.DataFrame(matrix[0,:], index=headlines, columns=[\"Score\"])\n",
    "df.sort_values(by=\"Score\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to switch from using local tools running solely on your machines to taking advantage of services, computational power and pre-trained models. We are going to rely on the Azure Cognitive Services Text Analytics tools. This does require you to sign up for a free account. There should be no charges.\n",
    "\n",
    "After you register, you will need to locate the TextAnalytics page on the Azure Portal to discover your user key and endpoint. Please paste those in below and uncomment the lines for the key and endpoint variable definitions. Do not check in keys like this to a public repository!\n",
    "\n",
    "Because the Azure services require authentication, the interaction is fairly complicated. To make this easier to use, Microsoft provides a series of language-specific clients that will shield you from this complexity if you use one. We are going to uses the Python Client but there are libraries for Java and C# too.\n",
    "\n",
    "To install the python client, you will need to run the following command:\n",
    "\n",
    "```pip install upgrade azure-cognitiveservices-language-textanalytics```\n",
    "\n",
    "You just need to run this cell to make the client set up available for our use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-c4660e59d808>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mazure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcognitiveservices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextanalytics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextAnalyticsClient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmsrest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauthentication\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCognitiveServicesCredentials\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'azure'"
     ]
    }
   ],
   "source": [
    "# key = ADD_YOUR_KEY_HERE\n",
    "# endpoint = ADD_YOUR_ENDPOINT_URL_HERE\n",
    "\n",
    "import os\n",
    "from azure.cognitiveservices.language.textanalytics import TextAnalyticsClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "def authenticateClient():\n",
    "    credentials = CognitiveServicesCredentials(key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=endpoint, credentials=credentials)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticateClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*INSTRUCTOR NOTE*: The final large exercise introduces the Azure Cognitive Services Text Analytics Services. While it is cool that open source tools such as NLTK and Sci-Kit Learn can be used by anyone to build up various sophisticated machine learning models for processing text, there are some operational considerations that should be considered. You may have more data to process than you have machines to use. Azure Cloud services can help spin up capacity as needed. More crucially, you may not have enough data to build sophisticated models. The Text Analytics Services let us leverage pre-trained models through convenient APIs with a capacity for growth in the computational demands. There is also nothing stopping you from mixing and matching these backend API services with locally running NLTK and Sci-Kit code.\n",
    "\n",
    "This exercise reuses our existing infrastructure, but the Azure Text Analytics services expect the data to be structured a little differently. We build up a keyed representation of the documents and accumulate more details as we use different services. For example, one service will tell us the language of the document (e.g. English, French, Japanese). We will capture these details and pass them on. Upstream systems might need to select different lexicons, parsing rules, stop word lists, etc. depending on the document's language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fetching: https://www.bbc.co.uk/news/technology-53425822\nFetching: https://www.bbc.co.uk/news/world-us-canada-53426285\nFetching: https://www.bbc.co.uk/news/business-53399999\nFetching: https://www.bbc.co.uk/news/world-us-canada-53423927\nFetching: https://www.bbc.co.uk/news/world-53420409\nFetching: https://www.bbc.co.uk/news/technology-53412678\nFetching: https://www.bbc.co.uk/news/world-us-canada-53425238\nFetching: https://www.bbc.co.uk/news/world-middle-east-53417228\nFetching: https://www.bbc.co.uk/news/world-europe-53415693\nFetching: https://www.bbc.co.uk/news/world-middle-east-53417227\nFetching: https://www.bbc.co.uk/news/world-europe-53424074\nFetching: https://www.bbc.co.uk/news/world-africa-53416277\nFetching: https://www.bbc.co.uk/news/world-53424726\nFetching: https://www.bbc.co.uk/news/world-53353748\nFetching: https://www.bbc.co.uk/news/world-asia-india-53361538\nFetching: https://www.bbc.co.uk/news/world-europe-53415780\n"
    },
    {
     "output_type": "error",
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 122457: character maps to <undefined>",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-1a6f2dace343>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Fetching: {post.link}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mentryFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetchAndProcessUrlIfNecessary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Some of the Cognitive Services have size limits on documents.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-0cc6488359b6>\u001b[0m in \u001b[0;36mfetchAndProcessUrlIfNecessary\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessedFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mentryFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             data = cleanMe(data,\n\u001b[0;32m     14\u001b[0m                 id_filters=[\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 122457: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# Retrieve a parsed version of our data source\n",
    "feed = fetchFeed()\n",
    "\n",
    "documents = []\n",
    "i = 0\n",
    "\n",
    "for post in feed.entries :\n",
    "    print(f\"Fetching: {post.link}\")\n",
    "\n",
    "    entryFile = fetchAndProcessUrlIfNecessary(post.link)\n",
    "\n",
    "    # Some of the Cognitive Services have size limits on documents. \n",
    "    # You can clearly break them into chunks and process them separately,\n",
    "    # but we are just going to grab a chunk below the threshold so we\n",
    "    # don't error out later.\n",
    "    if len(entryFile) > 5000 :\n",
    "        entryFile = entryFile[0:5000] \n",
    "\n",
    "    documents.append({'title' : post.title, 'text' : entryFile, \"id\" : i})\n",
    "    i += 1\n",
    "\n",
    "\n",
    "# Start processing the documents with the Azure Services\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# This service detects the language of the documents and returns the results\n",
    "# with the associated id so we can keep track. It is important to note that\n",
    "# this local method call triggers calls to backend services.\n",
    "\n",
    "response = client.detect_language(documents=documents)\n",
    "\n",
    "# We iterate over the results per document and update the associated document metadata.\n",
    "for document in response.documents:\n",
    "   documents[int(document.id)]['language'] = document.detected_languages[0].iso6391_name\n",
    "\n",
    "# This service issues an aggregate sentiment score between 0.0 (negative) and 1.0 (positive).\n",
    "# 0.5 represents a neutral sentiment. It is important to note that this local method call\n",
    "# triggers calls to backend services.\n",
    "\n",
    "response = client.sentiment(documents=documents)\n",
    "\n",
    "# We iterate over the results per document and update the associated document metadata.\n",
    "for document in response.documents:\n",
    "    documents[int(document.id)]['sentiment'] = document.score\n",
    "\n",
    "# One of the other useful services we can call on our documents involve extracting \n",
    "# key phrases from the text. This could be useful for categorizing and clustering\n",
    "# documents.\n",
    "response = client.key_phrases(documents=documents)\n",
    "\n",
    "# We iterate over the results per document and update the associated document metadata\n",
    "for document in response.documents:\n",
    "   documents[int(document.id)]['keyphrases'] = document.key_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is just a useful placeholder for exploring results. You can see what details we have learned per document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'title': \"Brad Parscale replaced as Trump's campaign manager\", 'text': 'Brad Parscale replaced as Trump\\'s campaign manager - BBC News\\nFacing a tough re-election campaign, US President Donald Trump has replaced his campaign manager.Mr Trump said he had substituted Bill Stepien, a field director for his 2016 campaign, in place of Brad Parscale.Mr Parscale - who was reportedly blamed by Mr Trump\\'s inner circle for a poorly attended rally in Oklahoma last month - will stay on as senior adviser.Opinion polls show the president is trailing his Democratic challenger Joe Biden ahead of November\\'s election.Mr Trump\\'s statement on Facebook on Wednesday evening said: \"Brad Parscale, who has been with me for a very long time and has led our tremendous digital and data strategies, will remain in that role, while being a Senior Advisor to the campaign.\"\\nMr Parscale is said to have found himself sidelined in recent weeks after the president\\'s comeback rally in Tulsa flopped.Mr Trump\\'s daughter Ivanka Trump and her husband Jared Kushner, both White House advisers, are reported to have blamed Mr Parscale for the debacle.Mr Parscale had boasted that more than one million people registered to attend, but fewer than 6,200 showed up at the arena, the local fire department said.After the rally, Mr Parscale went on Twitter to blame a blocked security gate, protesters and the media for the disappointing turnout.His role as a Trump strategist has apparently proved lucrative for the 44-year-old, who last year reportedly bought a $2.4m waterside mansion in Fort Lauderdale, Florida.Mr Parscale, a brash figure who served as a warm-up act for Mr Trump at rallies, was appointed campaign manager in February 2018.The BBC\\'s US partner CBS News reported recently that Mr Parscale did not even vote in the 2016 election, citing his difficulty in obtaining a postal ballot while working at Trump Tower in New York City.He was Mr Trump\\'s fourth campaign manager, following Kellyanne Conway, Paul Manafort and Corey Lewandowski.His replacement, Mr Stepien, was a former aide to ex-New Jersey Governor Chris Christie. that was widely perceived to have scuppered Mr Christie\\'s presidential ambitions.', 'id': 1}\n"
    }
   ],
   "source": [
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "The following exercise invokes behavior from the Azure Cognitive Services Text Analytics client to extract entity references from our documents. As we iterate over the response from the service (in the first line), we add the entity references to each of our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-a7a690db54e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# It is important to note that this local method call triggers calls to backend services.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# TODO: Handle errors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# This service extracts named entities from the document including names, type and subtype information.\n",
    "# It is important to note that this local method call triggers calls to backend services.\n",
    "\n",
    "response = client.entities(documents=documents)\n",
    "\n",
    "# TODO: Handle errors\n",
    "\n",
    "for document in response.documents:\n",
    "    entities = []\n",
    "\n",
    "    for entity in document.entities:\n",
    "        # It is completely arbitrary that we are filtering out these entity types. They are perfectly valid\n",
    "        # types and are likely to be of interest in processing financial news data.\n",
    "        if entity.type != 'Quantity' and entity.type != 'DateTime':\n",
    "            entities.append({'name' : entity.name, 'type' : entity.type, 'subtype' : entity.sub_type })\n",
    "\n",
    "    documents[int(document.id)]['entities'] = entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is a convenience for keeping track of sentiment references and the associated headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addReference(refs, key, sentiment, text):\n",
    "   currentRefs = refs.get(key, {'sentiment' : [], 'headline' : []})\n",
    "   currentRefs['sentiment'].append(document['sentiment'])\n",
    "   currentRefs['headline'].append(document['title'])\n",
    "   refs[key] = currentRefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two functions are convenience functions to calculate the average sentiment score and to summarize the details of a specific set of entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageSentimentValue(ratings) :\n",
    "    return np.array(ratings).mean()\n",
    "\n",
    "def entitySummary(entityType, refs, polarity):\n",
    "    print(f\"{entityType} with the most {polarity} coverage:\")\n",
    "\n",
    "    for entity in refs.keys() :\n",
    "        print('---------------')\n",
    "        print(f\"Name: {entity}\")\n",
    "        avg = averageSentimentValue(refs[entity]['sentiment'])\n",
    "        print(f\" .  Average sentiment analysis: {avg}\")\n",
    "        headlines = refs[entity]['headline']\n",
    "        print(\" .  Headlines:\")\n",
    "        for h in headlines:\n",
    "            print(f\"      {h}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell is going to add up the positive and negative entity references based on the articles they were mentioned in. The Text Analytics sentiment analysis service returns a value of 0.0 (very negative) to 1.0 (very positive) as a range. Neutral would register as 0.5. We picked >= 0.6 as the threshold for a positive mention and <= 0.4 for a negative mention. Feel free to play with those thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'entities'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-b355251a7f18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# modify the following code to track references for them too.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entities'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;31m# The previous steps captured entity names and types per document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentity\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'entities'"
     ]
    }
   ],
   "source": [
    "entityCount = {}\n",
    "entityTypeCount = {}\n",
    "posPersonRefs = {}\n",
    "negPersonRefs = {}\n",
    "posOrgRefs = {}\n",
    "negOrgRefs = {}\n",
    "\n",
    "# For each processed document, we look at the entities and their types as \n",
    "# identified by the Cognitive Services Text Analytics service. We add up\n",
    "# all of the Person and Organization entities that are referenced. We\n",
    "# limited it to those for this discussion of processing financial news as data.\n",
    "# There are certainly other entity types mentioned and you should feel free to\n",
    "# modify the following code to track references for them too.\n",
    "for document in documents:\n",
    "    for entity in document['entities']:\n",
    "        # The previous steps captured entity names and types per document\n",
    "        name = entity['name']\n",
    "        t = entity['type']\n",
    "\n",
    "        # We increment the references we see for\n",
    "        # each named entity and its type.\n",
    "        count = entityCount.get(name, 0)\n",
    "        entityCount[name] = count + 1\n",
    "        count = entityTypeCount.get(t, 0)\n",
    "        entityTypeCount[t] = count + 1\n",
    "\n",
    "        # This is an arbitrary threshold as mentioned above. Feel free to\n",
    "        # experiment with values to change the definition of a positive or\n",
    "        # negative reference.\n",
    "        if document['sentiment'] >= 0.6:\n",
    "            # Add the positive references\n",
    "            if t == 'Person' :\n",
    "                print(f\"Adding + person reference for {name}\")\n",
    "                addReference(posPersonRefs, name, document['sentiment'], document['title'])\n",
    "            elif t == 'Organization' :\n",
    "                print(f\"Adding + org reference for {name}\")\n",
    "                addReference(posOrgRefs, name, document['sentiment'], document['title'])\n",
    "\n",
    "        elif document['sentiment'] <= 0.4:\n",
    "            # Add the negative references\n",
    "            if t == 'Person' :\n",
    "                print(f\"Adding - person reference for {name}\")\n",
    "                addReference(negPersonRefs, name, document['sentiment'], document['title'])\n",
    "            elif t == 'Organization' :\n",
    "                print(f\"Adding - org reference for {name}\")\n",
    "                addReference(negOrgRefs, name, document['sentiment'], document['title'])\n",
    "                print(\"DONE ADDING REF\")\n",
    "\n",
    "            elif t == 'Organization' :\n",
    "                print(f\"Adding + org reference for {name}\")\n",
    "                addReference(posOrgRefs, name, document['sentiment'], document['title'])\n",
    "\n",
    "        elif document['sentiment'] <= 0.4:\n",
    "            # Add the negative references\n",
    "            if t == 'Person' :\n",
    "                print(f\"Adding - person reference for {name}\")\n",
    "                addReference(negPersonRefs, name, document['sentiment'], document['title'])\n",
    "            elif t == 'Organization' :\n",
    "                print(f\"Adding - org reference for {name}\")\n",
    "                addReference(negOrgRefs, name, document['sentiment'], document['title'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a convenience function to print out the top positive and negative references we saw across the corpus for People and Organizations. If you want to capture metrics about other entity types, you will most likely have to modify that part of the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "People with the most positive coverage:\nPeople with the most negative coverage:\nOrganizations with the most positive coverage:\nOrganizations with the most negative coverage:\n"
    }
   ],
   "source": [
    "entitySummary('People', posPersonRefs, 'positive')\n",
    "entitySummary('People', negPersonRefs, 'negative')\n",
    "entitySummary('Organizations', posOrgRefs, 'positive')\n",
    "entitySummary('Organizations', negOrgRefs, 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594867161210",
   "display_name": "Python 3.8.2 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}